{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b99704d5",
   "metadata": {},
   "source": [
    "## PT DDP Launcher Testing\n",
    "This notebook tests the following combination:\n",
    "\n",
    "* image: PT training DLC with my changes\n",
    "* distribution = pytorchddp, backend = nccl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd3f5d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07179f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./sagemaker-2.94.0.dev0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker==2.94.0.dev0) (1.19.2)\n",
      "Requirement already satisfied: attrs==20.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker==2.94.0.dev0) (20.3.0)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker==2.94.0.dev0) (0.1.5)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker==2.94.0.dev0) (3.19.1)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker==2.94.0.dev0) (1.1.5)\n",
      "Requirement already satisfied: boto3<2.0,>=1.20.21 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker==2.94.0.dev0) (1.21.42)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker==2.94.0.dev0) (1.0.1)\n",
      "Collecting importlib-metadata<2.0,>=1.4.0\n",
      "  Using cached importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker==2.94.0.dev0) (21.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker==2.94.0.dev0) (0.2.8)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker==2.94.0.dev0) (0.2.0)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.42 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3<2.0,>=1.20.21->sagemaker==2.94.0.dev0) (1.24.42)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3<2.0,>=1.20.21->sagemaker==2.94.0.dev0) (0.5.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3<2.0,>=1.20.21->sagemaker==2.94.0.dev0) (0.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata<2.0,>=1.4.0->sagemaker==2.94.0.dev0) (3.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==2.94.0.dev0) (2.4.7)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker==2.94.0.dev0) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pandas->sagemaker==2.94.0.dev0) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pandas->sagemaker==2.94.0.dev0) (2021.1)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pathos->sagemaker==2.94.0.dev0) (1.6.6.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pathos->sagemaker==2.94.0.dev0) (0.70.12.2)\n",
      "Requirement already satisfied: dill>=0.3.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pathos->sagemaker==2.94.0.dev0) (0.3.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pathos->sagemaker==2.94.0.dev0) (0.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from botocore<1.25.0,>=1.24.42->boto3<2.0,>=1.20.21->sagemaker==2.94.0.dev0) (1.26.8)\n",
      "Installing collected packages: importlib-metadata, sagemaker\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.7.0\n",
      "    Uninstalling importlib-metadata-3.7.0:\n",
      "      Successfully uninstalled importlib-metadata-3.7.0\n",
      "Successfully installed importlib-metadata-1.7.0 sagemaker-2.94.0.dev0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Upload sagemaker-python-sdk whl from local machine and install it here\n",
    "%pip install sagemaker-2.94.0.dev0-py2.py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf20495a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sagemaker\n",
      "Version: 2.94.0.dev0\n",
      "Summary: Open source library for training and deploying models on Amazon SageMaker.\n",
      "Home-page: https://github.com/aws/sagemaker-python-sdk/\n",
      "Author: Amazon Web Services\n",
      "Author-email: \n",
      "License: Apache License 2.0\n",
      "Location: /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages\n",
      "Requires: attrs, boto3, google-pasta, importlib-metadata, numpy, packaging, pandas, pathos, protobuf, protobuf3-to-dict, smdebug-rulesconfig\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e78ae0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::570106654206:role/Dev\n",
      "sagemaker bucket: sagemaker-us-west-2-570106654206\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "#Add instructions for local environment later, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ded02c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-west-2\"\n",
    "image = (\n",
    "    \"pt-ddp-custom\"  # Example: pt-smdataparallel-efficientnet-sagemaker\n",
    ")\n",
    "tag = \"latest\"  # Example: latest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676cf980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run only when docker push fails with OOM errors\n",
    "#! docker system prune -af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "380d5926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "! aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 570106654206.dkr.ecr.{region}.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fe818f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# refer https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-training-containers to get the right uri's based on region\n",
    "#image_uri = '763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.9.1-transformers4.12.3-gpu-py38-cu111-ubuntu20.04'\n",
    "#image_uri = '570106654206.dkr.ecr.us-west-2.amazonaws.com/ptddp-launcher:latest'\n",
    "#Using URI with logs added to DLC\n",
    "image_uri = '570106654206.dkr.ecr.us-west-2.amazonaws.com/pt-ddp-custom:latest'\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "# this is the only line of code change required to leverage SageMaker Distributed Data Parallel\n",
    "distribution = {'pytorchddp':{ 'enabled': True }}\n",
    "#distribution = {\"mpi\":{\"enabled\":True, \"num_of_processes_per_host\":8}}\n",
    "#distribution = { \"smdistributed\": { \"dataparallel\": { \"enabled\": True } } }\n",
    "\n",
    "\n",
    "estimator = PyTorch(\n",
    "    base_job_name=\"ptddp-mnist-test\",\n",
    "    source_dir=\"code\",\n",
    "    entry_point=\"train_ptddp_mnist.py\",\n",
    "    role=role,\n",
    "    framework_version=\"1.11.0\",\n",
    "    py_version=\"py38\",\n",
    "    image_uri=image_uri,\n",
    "    # For training with multinode distributed training, set this count. Example: 2\n",
    "    instance_count=1,\n",
    "    # For training with p3dn instance use - ml.p3dn.24xlarge, with p4dn instance use - ml.p4d.24xlarge\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    sagemaker_session=sess,\n",
    "    # Training using SMDataParallel Distributed Training Framework\n",
    "    distribution=distribution,\n",
    "    debugger_hook_config=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a147ad54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-01 18:05:04 Starting - Starting the training job...ProfilerReport-1656698704: InProgress\n",
      "...\n",
      "2022-07-01 18:05:48 Starting - Preparing the instances for training..........................................\n",
      "2022-07-01 18:13:06 Downloading - Downloading input data\n",
      "2022-07-01 18:13:06 Training - Downloading the training image...........................\n",
      "2022-07-01 18:17:32 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34msed: can't read changehostname.c: No such file or directory\u001b[0m\n",
      "\u001b[34mgcc: error: changehostname.c: No such file or directory\u001b[0m\n",
      "\u001b[34mgcc: fatal error: no input files\u001b[0m\n",
      "\u001b[34mcompilation terminated.\u001b[0m\n",
      "\u001b[34mgcc: error: changehostname.o: No such file or directory\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/paramiko/transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m2022-07-01 18:17:35,082 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m2022-07-01 18:17:35,158 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-07-01 18:17:35,158 sagemaker_pytorch_container.training INFO     Calling viskaria fork of pt training toolkit.\u001b[0m\n",
      "\u001b[34m2022-07-01 18:17:35,164 sagemaker_pytorch_container.training INFO     viskaria pytorch_ddp_enabled is: True\u001b[0m\n",
      "\u001b[34m2022-07-01 18:17:35,164 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel for native PT DDP job\u001b[0m\n",
      "\u001b[34m2022-07-01 18:17:35,164 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m2022-07-01 18:17:35,695 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2022-07-01 18:17:35,695 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m2022-07-01 18:17:35,698 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2022-07-01 18:17:35,699 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2022-07-01 18:17:35,699 sagemaker-training-toolkit INFO     Host: ['algo-1']\u001b[0m\n",
      "\u001b[34m2022-07-01 18:17:35,701 sagemaker-training-toolkit INFO     instance type: None\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m2022-07-01 18:17:35,779 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_pytorch_ddp_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"ptddp-mnist-test-2022-07-01-18-05-04-221\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-07-01-18-05-04-221/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_ptddp_mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_ptddp_mnist.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_ptddp_mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_pytorch_ddp_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_ptddp_mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-07-01-18-05-04-221/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_pytorch_ddp_enabled\":true},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"ptddp-mnist-test-2022-07-01-18-05-04-221\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-07-01-18-05-04-221/source/sourcedir.tar.gz\",\"module_name\":\"train_ptddp_mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_ptddp_mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220430-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1 -np 8 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so smddprun /opt/conda/bin/python3.8 -m mpi4py train_ptddp_mnist.py\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:All environment variables are:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:All environment variables are:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:All environment variables are:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:All environment variables are:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:All environment variables are:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:All environment variables are:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:All environment variables are:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:All environment variables are:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:environ({'OMPI_MCA_orte_app_num': '0', 'NVIDIA_VISIBLE_DEVICES': 'all', 'OMPI_MCA_btl_vader_single_copy_mechanism': 'none', 'SAGEMAKER_JOB_NAME': 'ptddp-mnist-test-2022-07-01-18-05-04-221', 'SM_CHANNELS': '[]', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'OMPI_MCA_orte_precondition_transports': '0640943619e0ff47-51ad4c4750a63c17', 'PYTHONUNBUFFERED': '1', 'OMPI_MCA_shmem_RUNTIME_QUERY_hint': 'mmap', 'OMPI_UNIVERSE_SIZE': '1', 'OMPI_MCA_rmaps_base_oversubscribe': '1', 'PMIX_MCA_mca_base_component_show_load_errors': '1', 'SM_HOSTS': '[\"algo-1\"]', 'LOCAL_RANK': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-44661a7fb67e0f6fd65efc5a81fad538b526dfd3230e82f16e907e32fb8c62fb-customer', 'PMIX_HOSTNAME': 'ip-10-0-216-15.us-west-2.compute.internal', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'HOSTNAME': 'ip-10-0-216-15.us-west-2.compute.internal', 'OMPI_COMM_WORLD_NODE_RANK': '0', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'OMPI_MCA_ess_base_vpid': '0', 'SM_MODULE_NAME': 'train_ptddp_mnist', 'PMIX_SECURITY_MODE': 'native', 'MASTER_PORT': '7777', 'OMPI_MCA_orte_hnp_uri': '1841889280.0;tcp://10.0.216.15:47141', 'NVIDIA_REQUIRE_CUDA': 'cuda>=11.3 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=440,driver<441 driver>=450', 'OMPI_MCA_plm_rsh_num_concurrent': '1', 'MANUAL_BUILD': '0', 'NV_ML_REPO_ENABLED': '1', 'BRANCH_OFI': '1.2.0-aws', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'OMPI_MCA_btl': '^openib', 'OMPI_COMM_WORLD_LOCAL_RANK': '0', 'TORCH_CUDA_ARCH_LIST': '3.7 5.0 7.0+PTX 8.0', 'NCCL_VERSION': '2.10.3', 'NCCL_SOCKET_IFNAME': 'eth0', 'HFI_NO_BACKTRACE': '1', 'AWS_REGION': 'us-west-2', 'PMIX_GDS_MODULE': 'ds21,ds12,hash', 'SMDATAPARALLEL_WORLD_RANK': '0', 'PWD': '/opt/ml/code', 'OMPI_MCA_orte_abort_on_non_zero_status': '1', 'OMPI_MCA_orte_local_daemon_uri': '1841889280.0;tcp://10.0.216.15:47141', 'RDMAV_FORK_SAFE': '1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video', 'OMPI_MCA_oob_tcp_if_include': 'eth0', 'NCCL_DEBUG': 'INFO', 'OMPI_MCA_pml': 'ob1', 'HOROVOD_VERSION': '0.24.3', 'PMIX_SYSTEM_TMPDIR': '/tmp', 'WORLD_SIZE': '8', 'SM_USER_ENTRY_POINT': 'train_ptddp_mnist.py', 'OPEN_MPI_PATH': '/opt/amazon/openmpi', 'OMPI_ARGV': '/opt/conda/bin/python3.8 -m mpi4py train_ptddp_mnist.py', 'PMIX_BFROP_BUFFER_TYPE': 'PMIX_BFROP_BUFFER_NON_DESC', 'LD_PRELOAD': '/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so', 'NV_CUDA_CUDART_VERSION': '11.3.109-1', 'PMIX_PTL_MODULE': 'tcp,usock', 'HOME': '/root', 'FI_PROVIDER': 'efa', 'OMPI_NUM_APP_CTX': '1', 'LANG': 'C.UTF-8', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'PMIX_DSTORE_21_BASE_PATH': '/tmp/ompi.ip-10-0-216-15.0/pid.24/pmix_dstor_ds21_24', 'OMPI_MCA_orte_num_nodes': '1', 'PMIX_VERSION': '3.2.3', 'SMDATAPARALLEL_LOCAL_RANK': '0', 'CUDA_VERSION': '11.3.1', 'DMLC_INTERFACE': 'eth0', 'OMPI_MCA_orte_tag_output': '1', 'SM_NUM_CPUS': '96', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'DGLBACKEND': 'pytorch', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'USE_SMDEBUG': '0', 'OMPI_MCA_orte_tmpdir_base': '/tmp', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'PMIX_RANK': '0', 'PMIX_SERVER_URI2': '1841889280.0;tcp4://127.0.0.1:50901', 'PMIX_SERVER_URI3': '1841889280.0;tcp4://127.0.0.1:50901', 'SM_CURRENT_HOST': 'algo-1', 'OMPI_MCA_initial_wdir': '/opt/ml/code', 'OMPI_FIRST_RANKS': '0', 'OMPI_MCA_mpi_oversubscribe': '1', 'SMDATAPARALLEL_WORLD_SIZE': '8', 'OMPI_MCA_orte_jobfam_session_dir': '/tmp/ompi.ip-10-0-216-15.0/pid.24', 'MASTER_ADDR': 'algo-1', 'SM_INPUT_DATA_CONFIG': '{}', 'SM_NUM_GPUS': '8', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'PYTHONPA[1,mpirank:1,algo-1]<stdout>:environ({'OMPI_MCA_orte_app_num': '0', 'NVIDIA_VISIBLE_DEVICES': 'all', 'OMPI_MCA_btl_vader_single_copy_mechanism': 'none', 'SAGEMAKER_JOB_NAME': 'ptddp-mnist-test-2022-07-01-18-05-04-221', 'SM_CHANNELS': '[]', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'OMPI_MCA_orte_precondition_transports': '0640943619e0ff47-51ad4c4750a63c17', 'PYTHONUNBUFFERED': '1', 'OMPI_MCA_shmem_RUNTIME_QUERY_hint': 'mmap', 'OMPI_UNIVERSE_SIZE': '1', 'OMPI_MCA_rmaps_base_oversubscribe': '1', 'PMIX_MCA_mca_base_component_show_load_errors': '1', 'SM_HOSTS': '[\"algo-1\"]', 'LOCAL_RANK': '1', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-44661a7fb67e0f6fd65efc5a81fad538b526dfd3230e82f16e907e32fb8c62fb-customer', 'PMIX_HOSTNAME': 'ip-10-0-216-15.us-west-2.compute.internal', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'HOSTNAME': 'ip-10-0-216-15.us-west-2.compute.internal', 'OMPI_COMM_WORLD_NODE_RANK': '1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'OMPI_MCA_ess_base_vpid': '1', 'SM_MODULE_NAME': 'train_ptddp_mnist', 'PMIX_SECURITY_MODE': 'native', 'MASTER_PORT': '7777', 'OMPI_MCA_orte_hnp_uri': '1841889280.0;tcp://10.0.216.15:47141', 'NVIDIA_REQUIRE_CUDA': 'cuda>=11.3 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=440,driver<441 driver>=450', 'OMPI_MCA_plm_rsh_num_concurrent': '1', 'MANUAL_BUILD': '0', 'NV_ML_REPO_ENABLED': '1', 'BRANCH_OFI': '1.2.0-aws', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'OMPI_MCA_btl': '^openib', 'OMPI_COMM_WORLD_LOCAL_RANK': '1', 'TORCH_CUDA_ARCH_LIST': '3.7 5.0 7.0+PTX 8.0', 'NCCL_VERSION': '2.10.3', 'NCCL_SOCKET_IFNAME': 'eth0', 'HFI_NO_BACKTRACE': '1', 'AWS_REGION': 'us-west-2', 'PMIX_GDS_MODULE': 'ds21,ds12,hash', 'SMDATAPARALLEL_WORLD_RANK': '1', 'PWD': '/opt/ml/code', 'OMPI_MCA_orte_abort_on_non_zero_status': '1', 'OMPI_MCA_orte_local_daemon_uri': '1841889280.0;tcp://10.0.216.15:47141', 'RDMAV_FORK_SAFE': '1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video', 'OMPI_MCA_oob_tcp_if_include': 'eth0', 'NCCL_DEBUG': 'INFO', 'OMPI_MCA_pml': 'ob1', 'HOROVOD_VERSION': '0.24.3', 'PMIX_SYSTEM_TMPDIR': '/tmp', 'WORLD_SIZE': '8', 'SM_USER_ENTRY_POINT': 'train_ptddp_mnist.py', 'OPEN_MPI_PATH': '/opt/amazon/openmpi', 'OMPI_ARGV': '/opt/conda/bin/python3.8 -m mpi4py train_ptddp_mnist.py', 'PMIX_BFROP_BUFFER_TYPE': 'PMIX_BFROP_BUFFER_NON_DESC', 'LD_PRELOAD': '/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so', 'NV_CUDA_CUDART_VERSION': '11.3.109-1', 'PMIX_PTL_MODULE': 'tcp,usock', 'HOME': '/root', 'FI_PROVIDER': 'efa', 'OMPI_NUM_APP_CTX': '1', 'LANG': 'C.UTF-8', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'PMIX_DSTORE_21_BASE_PATH': '/tmp/ompi.ip-10-0-216-15.0/pid.24/pmix_dstor_ds21_24', 'OMPI_MCA_orte_num_nodes': '1', 'PMIX_VERSION': '3.2.3', 'SMDATAPARALLEL_LOCAL_RANK': '1', 'CUDA_VERSION': '11.3.1', 'DMLC_INTERFACE': 'eth0', 'OMPI_MCA_orte_tag_output': '1', 'SM_NUM_CPUS': '96', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'DGLBACKEND': 'pytorch', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'USE_SMDEBUG': '0', 'OMPI_MCA_orte_tmpdir_base': '/tmp', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'PMIX_RANK': '1', 'PMIX_SERVER_URI2': '1841889280.0;tcp4://127.0.0.1:50901', 'PMIX_SERVER_URI3': '1841889280.0;tcp4://127.0.0.1:50901', 'SM_CURRENT_HOST': 'algo-1', 'OMPI_MCA_initial_wdir': '/opt/ml/code', 'OMPI_FIRST_RANKS': '0', 'OMPI_MCA_mpi_oversubscribe': '1', 'SMDATAPARALLEL_WORLD_SIZE': '8', 'OMPI_MCA_orte_jobfam_session_dir': '/tmp/ompi.ip-10-0-216-15.0/pid.24', 'MASTER_ADDR': 'algo-1', 'SM_INPUT_DATA_CONFIG': '{}', 'SM_NUM_GPUS': '8', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'PYTHONPA[1,mpirank:3,algo-1]<stdout>:environ({'OMPI_MCA_orte_app_num': '0', 'NVIDIA_VISIBLE_DEVICES': 'all', 'OMPI_MCA_btl_vader_single_copy_mechanism': 'none', 'SAGEMAKER_JOB_NAME': 'ptddp-mnist-test-2022-07-01-18-05-04-221', 'SM_CHANNELS': '[]', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'OMPI_MCA_orte_precondition_transports': '0640943619e0ff47-51ad4c4750a63c17', 'PYTHONUNBUFFERED': '1', 'OMPI_MCA_shmem_RUNTIME_QUERY_hint': 'mmap', 'OMPI_UNIVERSE_SIZE': '1', 'OMPI_MCA_rmaps_base_oversubscribe': '1', 'PMIX_MCA_mca_base_component_show_load_errors': '1', 'SM_HOSTS': '[\"algo-1\"]', 'LOCAL_RANK': '3', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-44661a7fb67e0f6fd65efc5a81fad538b526dfd3230e82f16e907e32fb8c62fb-customer', 'PMIX_HOSTNAME': 'ip-10-0-216-15.us-west-2.compute.internal', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'HOSTNAME': 'ip-10-0-216-15.us-west-2.compute.internal', 'OMPI_COMM_WORLD_NODE_RANK': '3', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'OMPI_MCA_ess_base_vpid': '3', 'SM_MODULE_NAME': 'train_ptddp_mnist', 'PMIX_SECURITY_MODE': 'native', 'MASTER_PORT': '7777', 'OMPI_MCA_orte_hnp_uri': '1841889280.0;tcp://10.0.216.15:47141', 'NVIDIA_REQUIRE_CUDA': 'cuda>=11.3 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=440,driver<441 driver>=450', 'OMPI_MCA_plm_rsh_num_concurrent': '1', 'MANUAL_BUILD': '0', 'NV_ML_REPO_ENABLED': '1', 'BRANCH_OFI': '1.2.0-aws', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'OMPI_MCA_btl': '^openib', 'OMPI_COMM_WORLD_LOCAL_RANK': '3', 'TORCH_CUDA_ARCH_LIST': '3.7 5.0 7.0+PTX 8.0', 'NCCL_VERSION': '2.10.3', 'NCCL_SOCKET_IFNAME': 'eth0', 'HFI_NO_BACKTRACE': '1', 'AWS_REGION': 'us-west-2', 'PMIX_GDS_MODULE': 'ds21,ds12,hash', 'SMDATAPARALLEL_WORLD_RANK': '3', 'PWD': '/opt/ml/code', 'OMPI_MCA_orte_abort_on_non_zero_status': '1', 'OMPI_MCA_orte_local_daemon_uri': '1841889280.0;tcp://10.0.216.15:47141', 'RDMAV_FORK_SAFE': '1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video', 'OMPI_MCA_oob_tcp_if_include': 'eth0', 'NCCL_DEBUG': 'INFO', 'OMPI_MCA_pml': 'ob1', 'HOROVOD_VERSION': '0.24.3', 'PMIX_SYSTEM_TMPDIR': '/tmp', 'WORLD_SIZE': '8', 'SM_USER_ENTRY_POINT': 'train_ptddp_mnist.py', 'OPEN_MPI_PATH': '/opt/amazon/openmpi', 'OMPI_ARGV': '/opt/conda/bin/python3.8 -m mpi4py train_ptddp_mnist.py', 'PMIX_BFROP_BUFFER_TYPE': 'PMIX_BFROP_BUFFER_NON_DESC', 'LD_PRELOAD': '/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so', 'NV_CUDA_CUDART_VERSION': '11.3.109-1', 'PMIX_PTL_MODULE': 'tcp,usock', 'HOME': '/root', 'FI_PROVIDER': 'efa', 'OMPI_NUM_APP_CTX': '1', 'LANG': 'C.UTF-8', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'PMIX_DSTORE_21_BASE_PATH': '/tmp/ompi.ip-10-0-216-15.0/pid.24/pmix_dstor_ds21_24', 'OMPI_MCA_orte_num_nodes': '1', 'PMIX_VERSION': '3.2.3', 'SMDATAPARALLEL_LOCAL_RANK': '3', 'CUDA_VERSION': '11.3.1', 'DMLC_INTERFACE': 'eth0', 'OMPI_MCA_orte_tag_output': '1', 'SM_NUM_CPUS': '96', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'DGLBACKEND': 'pytorch', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'USE_SMDEBUG': '0', 'OMPI_MCA_orte_tmpdir_base': '/tmp', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'PMIX_RANK': '3', 'PMIX_SERVER_URI2': '1841889280.0;tcp4://127.0.0.1:50901', 'PMIX_SERVER_URI3': '1841889280.0;tcp4://127.0.0.1:50901', 'SM_CURRENT_HOST': 'algo-1', 'OMPI_MCA_initial_wdir': '/opt/ml/code', 'OMPI_FIRST_RANKS': '0', 'OMPI_MCA_mpi_oversubscribe': '1', 'SMDATAPARALLEL_WORLD_SIZE': '8', 'OMPI_MCA_orte_jobfam_session_dir': '/tmp/ompi.ip-10-0-216-15.0/pid.24', 'MASTER_ADDR': 'algo-1', 'SM_INPUT_DATA_CONFIG': '{}', 'SM_NUM_GPUS': '8', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'PYTHONPA[1,mpirank:4,algo-1]<stdout>:environ({'OMPI_MCA_orte_app_num': '0', 'NVIDIA_VISIBLE_DEVICES': 'all', 'OMPI_MCA_btl_vader_single_copy_mechanism': 'none', 'SAGEMAKER_JOB_NAME': 'ptddp-mnist-test-2022-07-01-18-05-04-221', 'SM_CHANNELS': '[]', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'OMPI_MCA_orte_precondition_transports': '0640943619e0ff47-51ad4c4750a63c17', 'PYTHONUNBUFFERED': '1', 'OMPI_MCA_shmem_RUNTIME_QUERY_hint': 'mmap', 'OMPI_UNIVERSE_SIZE': '1', 'OMPI_MCA_rmaps_base_oversubscribe': '1', 'PMIX_MCA_mca_base_component_show_load_errors': '1', 'SM_HOSTS': '[\"algo-1\"]', 'LOCAL_RANK': '4', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-44661a7fb67e0f6fd65efc5a81fad538b526dfd3230e82f16e907e32fb8c62fb-customer', 'PMIX_HOSTNAME': 'ip-10-0-216-15.us-west-2.compute.internal', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'HOSTNAME': 'ip-10-0-216-15.us-west-2.compute.internal', 'OMPI_COMM_WORLD_NODE_RANK': '4', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'OMPI_MCA_ess_base_vpid': '4', 'SM_MODULE_NAME': 'train_ptddp_mnist', 'PMIX_SECURITY_MODE': 'native', 'MASTER_PORT': '7777', 'OMPI_MCA_orte_hnp_uri': '1841889280.0;tcp://10.0.216.15:47141', 'NVIDIA_REQUIRE_CUDA': 'cuda>=11.3 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=440,driver<441 driver>=450', 'OMPI_MCA_plm_rsh_num_concurrent': '1', 'MANUAL_BUILD': '0', 'NV_ML_REPO_ENABLED': '1', 'BRANCH_OFI': '1.2.0-aws', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'OMPI_MCA_btl': '^openib', 'OMPI_COMM_WORLD_LOCAL_RANK': '4', 'TORCH_CUDA_ARCH_LIST': '3.7 5.0 7.0+PTX 8.0', 'NCCL_VERSION': '2.10.3', 'NCCL_SOCKET_IFNAME': 'eth0', 'HFI_NO_BACKTRACE': '1', 'AWS_REGION': 'us-west-2', 'PMIX_GDS_MODULE': 'ds21,ds12,hash', 'SMDATAPARALLEL_WORLD_RANK': '4', 'PWD': '/opt/ml/code', 'OMPI_MCA_orte_abort_on_non_zero_status': '1', 'OMPI_MCA_orte_local_daemon_uri': '1841889280.0;tcp://10.0.216.15:47141', 'RDMAV_FORK_SAFE': '1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video', 'OMPI_MCA_oob_tcp_if_include': 'eth0', 'NCCL_DEBUG': 'INFO', 'OMPI_MCA_pml': 'ob1', 'HOROVOD_VERSION': '0.24.3', 'PMIX_SYSTEM_TMPDIR': '/tmp', 'WORLD_SIZE': '8', 'SM_USER_ENTRY_POINT': 'train_ptddp_mnist.py', 'OPEN_MPI_PATH': '/opt/amazon/openmpi', 'OMPI_ARGV': '/opt/conda/bin/python3.8 -m mpi4py train_ptddp_mnist.py', 'PMIX_BFROP_BUFFER_TYPE': 'PMIX_BFROP_BUFFER_NON_DESC', 'LD_PRELOAD': '/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so', 'NV_CUDA_CUDART_VERSION': '11.3.109-1', 'PMIX_PTL_MODULE': 'tcp,usock', 'HOME': '/root', 'FI_PROVIDER': 'efa', 'OMPI_NUM_APP_CTX': '1', 'LANG': 'C.UTF-8', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'PMIX_DSTORE_21_BASE_PATH': '/tmp/ompi.ip-10-0-216-15.0/pid.24/pmix_dstor_ds21_24', 'OMPI_MCA_orte_num_nodes': '1', 'PMIX_VERSION': '3.2.3', 'SMDATAPARALLEL_LOCAL_RANK': '4', 'CUDA_VERSION': '11.3.1', 'DMLC_INTERFACE': 'eth0', 'OMPI_MCA_orte_tag_output': '1', 'SM_NUM_CPUS': '96', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'DGLBACKEND': 'pytorch', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'USE_SMDEBUG': '0', 'OMPI_MCA_orte_tmpdir_base': '/tmp', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'PMIX_RANK': '4', 'PMIX_SERVER_URI2': '1841889280.0;tcp4://127.0.0.1:50901', 'PMIX_SERVER_URI3': '1841889280.0;tcp4://127.0.0.1:50901', 'SM_CURRENT_HOST': 'algo-1', 'OMPI_MCA_initial_wdir': '/opt/ml/code', 'OMPI_FIRST_RANKS': '0', 'OMPI_MCA_mpi_oversubscribe': '1', 'SMDATAPARALLEL_WORLD_SIZE': '8', 'OMPI_MCA_orte_jobfam_session_dir': '/tmp/ompi.ip-10-0-216-15.0/pid.24', 'MASTER_ADDR': 'algo-1', 'SM_INPUT_DATA_CONFIG': '{}', 'SM_NUM_GPUS': '8', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"inst\u001b[0m\n",
      "\u001b[34mance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'PYTHONPA[1,mpirank:5,algo-1]<stdout>:environ({'OMPI_MCA_orte_app_num': '0', 'NVIDIA_VISIBLE_DEVICES': 'all', 'OMPI_MCA_btl_vader_single_copy_mechanism': 'none', 'SAGEMAKER_JOB_NAME': 'ptddp-mnist-test-2022-07-01-18-05-04-221', 'SM_CHANNELS': '[]', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'OMPI_MCA_orte_precondition_transports': '0640943619e0ff47-51ad4c4750a63c17', 'PYTHONUNBUFFERED': '1', 'OMPI_MCA_shmem_RUNTIME_QUERY_hint': 'mmap', 'OMPI_UNIVERSE_SIZE': '1', 'OMPI_MCA_rmaps_base_oversubscribe': '1', 'PMIX_MCA_mca_base_component_show_load_errors': '1', 'SM_HOSTS': '[\"algo-1\"]', 'LOCAL_RANK': '5', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-44661a7fb67e0f6fd65efc5a81fad538b526dfd3230e82f16e907e32fb8c62fb-customer', 'PMIX_HOSTNAME': 'ip-10-0-216-15.us-west-2.compute.internal', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'HOSTNAME': 'ip-10-0-216-15.us-west-2.compute.internal', 'OMPI_COMM_WORLD_NODE_RANK': '5', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'OMPI_MCA_ess_base_vpid': '5', 'SM_MODULE_NAME': 'train_ptddp_mnist', 'PMIX_SECURITY_MODE': 'native', 'MASTER_PORT': '7777', 'OMPI_MCA_orte_hnp_uri': '1841889280.0;tcp://10.0.216.15:47141', 'NVIDIA_REQUIRE_CUDA': 'cuda>=11.3 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=440,driver<441 driver>=450', 'OMPI_MCA_plm_rsh_num_concurrent': '1', 'MANUAL_BUILD': '0', 'NV_ML_REPO_ENABLED': '1', 'BRANCH_OFI': '1.2.0-aws', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'OMPI_MCA_btl': '^openib', 'OMPI_COMM_WORLD_LOCAL_RANK': '5', 'TORCH_CUDA_ARCH_LIST': '3.7 5.0 7.0+PTX 8.0', 'NCCL_VERSION': '2.10.3', 'NCCL_SOCKET_IFNAME': 'eth0', 'HFI_NO_BACKTRACE': '1', 'AWS_REGION': 'us-west-2', 'PMIX_GDS_MODULE': 'ds21,ds12,hash', 'SMDATAPARALLEL_WORLD_RANK': '5', 'PWD': '/opt/ml/code', 'OMPI_MCA_orte_abort_on_non_zero_status': '1', 'OMPI_MCA_orte_local_daemon_uri': '1841889280.0;tcp://10.0.216.15:47141', 'RDMAV_FORK_SAFE': '1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video', 'OMPI_MCA_oob_tcp_if_include': 'eth0', 'NCCL_DEBUG': 'INFO', 'OMPI_MCA_pml': 'ob1', 'HOROVOD_VERSION': '0.24.3', 'PMIX_SYSTEM_TMPDIR': '/tmp', 'WORLD_SIZE': '8', 'SM_USER_ENTRY_POINT': 'train_ptddp_mnist.py', 'OPEN_MPI_PATH': '/opt/amazon/openmpi', 'OMPI_ARGV': '/opt/conda/bin/python3.8 -m mpi4py train_ptddp_mnist.py', 'PMIX_BFROP_BUFFER_TYPE': 'PMIX_BFROP_BUFFER_NON_DESC', 'LD_PRELOAD': '/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so', 'NV_CUDA_CUDART_VERSION': '11.3.109-1', 'PMIX_PTL_MODULE': 'tcp,usock', 'HOME': '/root', 'FI_PROVIDER': 'efa', 'OMPI_NUM_APP_CTX': '1', 'LANG': 'C.UTF-8', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'PMIX_DSTORE_21_BASE_PATH': '/tmp/ompi.ip-10-0-216-15.0/pid.24/pmix_dstor_ds21_24', 'OMPI_MCA_orte_num_nodes': '1', 'PMIX_VERSION': '3.2.3', 'SMDATAPARALLEL_LOCAL_RANK': '5', 'CUDA_VERSION': '11.3.1', 'DMLC_INTERFACE': 'eth0', 'OMPI_MCA_orte_tag_output': '1', 'SM_NUM_CPUS': '96', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'DGLBACKEND': 'pytorch', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'USE_SMDEBUG': '0', 'OMPI_MCA_orte_tmpdir_base': '/tmp', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'PMIX_RANK': '5', 'PMIX_SERVER_URI2': '1841889280.0;tcp4://127.0.0.1:50901', 'PMIX_SERVER_URI3': '1841889280.0;tcp4://127.0.0.1:50901', 'SM_CURRENT_HOST': 'algo-1', 'OMPI_MCA_initial_wdir': '/opt/ml/code', 'OMPI_FIRST_RANKS': '0', 'OMPI_MCA_mpi_oversubscribe': '1', 'SMDATAPARALLEL_WORLD_SIZE': '8', 'OMPI_MCA_orte_jobfam_session_dir': '/tmp/ompi.ip-10-0-216-15.0/pid.24', 'MASTER_ADDR': 'algo-1', 'SM_INPUT_DATA_CONFIG': '{}', 'SM_NUM_GPUS': '8', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'PYTHONPA[1,mpirank:6,algo-1]<stdout>:environ({'OMPI_MCA_orte_app_num': '0', 'NVIDIA_VISIBLE_DEVICES': 'all', 'OMPI_MCA_btl_vader_single_copy_mechanism': 'none', 'SAGEMAKER_JOB_NAME': 'ptddp-mnist-test-2022-07-01-18-05-04-221', 'SM_CHANNELS': '[]', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'OMPI_MCA_orte_precondition_transports': '0640943619e0ff47-51ad4c4750a63c17', 'PYTHONUNBUFFERED': '1', 'OMPI_MCA_shmem_RUNTIME_QUERY_hint': 'mmap', 'OMPI_UNIVERSE_SIZE': '1', 'OMPI_MCA_rmaps_base_oversubscribe': '1', 'PMIX_MCA_mca_base_component_show_load_errors': '1', 'SM_HOSTS': '[\"algo-1\"]', 'LOCAL_RANK': '6', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-44661a7fb67e0f6fd65efc5a81fad538b526dfd3230e82f16e907e32fb8c62fb-customer', 'PMIX_HOSTNAME': 'ip-10-0-216-15.us-west-2.compute.internal', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'HOSTNAME': 'ip-10-0-216-15.us-west-2.compute.internal', 'OMPI_COMM_WORLD_NODE_RANK': '6', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'OMPI_MCA_ess_base_vpid': '6', 'SM_MODULE_NAME': 'train_ptddp_mnist', 'PMIX_SECURITY_MODE': 'native', 'MASTER_PORT': '7777', 'OMPI_MCA_orte_hnp_uri': '1841889280.0;tcp://10.0.216.15:47141', 'NVIDIA_REQUIRE_CUDA': 'cuda>=11.3 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=440,driver<441 driver>=450', 'OMPI_MCA_plm_rsh_num_concurrent': '1', 'MANUAL_BUILD': '0', 'NV_ML_REPO_ENABLED': '1', 'BRANCH_OFI': '1.2.0-aws', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'OMPI_MCA_btl': '^openib', 'OMPI_COMM_WORLD_LOCAL_RANK': '6', 'TORCH_CUDA_ARCH_LIST': '3.7 5.0 7.0+PTX 8.0', 'NCCL_VERSION': '2.10.3', 'NCCL_SOCKET_IFNAME': 'eth0', 'HFI_NO_BACKTRACE': '1', 'AWS_REGION': 'us-west-2', 'PMIX_GDS_MODULE': 'ds21,ds12,hash', 'SMDATAPARALLEL_WORLD_RANK': '6', 'PWD': '/opt/ml/code', 'OMPI_MCA_orte_abort_on_non_zero_status': '1', 'OMPI_MCA_orte_local_daemon_uri': '1841889280.0;tcp://10.0.216.15:47141', 'RDMAV_FORK_SAFE': '1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video', 'OMPI_MCA_oob_tcp_if_include': 'eth0', 'NCCL_DEBUG': 'INFO', 'OMPI_MCA_pml': 'ob1', 'HOROVOD_VERSION': '0.24.3', 'PMIX_SYSTEM_TMPDIR': '/tmp', 'WORLD_SIZE': '8', 'SM_USER_ENTRY_POINT': 'train_ptddp_mnist.py', 'OPEN_MPI_PATH': '/opt/amazon/openmpi', 'OMPI_ARGV': '/opt/conda/bin/python3.8 -m mpi4py train_ptddp_mnist.py', 'PMIX_BFROP_BUFFER_TYPE': 'PMIX_BFROP_BUFFER_NON_DESC', 'LD_PRELOAD': '/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so', 'NV_CUDA_CUDART_VERSION': '11.3.109-1', 'PMIX_PTL_MODULE': 'tcp,usock', 'HOME': '/root', 'FI_PROVIDER': 'efa', 'OMPI_NUM_APP_CTX': '1', 'LANG': 'C.UTF-8', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'PMIX_DSTORE_21_BASE_PATH': '/tmp/ompi.ip-10-0-216-15.0/pid.24/pmix_dstor_ds21_24', 'OMPI_MCA_orte_num_nodes': '1', 'PMIX_VERSION': '3.2.3', 'SMDATAPARALLEL_LOCAL_RANK': '6', 'CUDA_VERSION': '11.3.1', 'DMLC_INTERFACE': 'eth0', 'OMPI_MCA_orte_tag_output': '1', 'SM_NUM_CPUS': '96', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'DGLBACKEND': 'pytorch', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'USE_SMDEBUG': '0', 'OMPI_MCA_orte_tmpdir_base': '/tmp', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'PMIX_RANK': '6', 'PMIX_SERVER_URI2': '1841889280.0;tcp4://127.0.0.1:50901', 'PMIX_SERVER_URI3': '1841889280.0;tcp4://127.0.0.1:50901', 'SM_CURRENT_HOST': 'algo-1', 'OMPI_MCA_initial_wdir': '/opt/ml/code', 'OMPI_FIRST_RANKS': '0', 'OMPI_MCA_mpi_oversubscribe': '1', 'SMDATAPARALLEL_WORLD_SIZE': '8', 'OMPI_MCA_orte_jobfam_session_dir': '/tmp/ompi.ip-10-0-216-15.0/pid.24', 'MASTER_ADDR': 'algo-1', 'SM_INPUT_DATA_CONFIG': '{}', 'SM_NUM_GPUS': '8', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'PYTHONPA[1,mpirank:7,algo-1]<stdout>:environ({'OMPI_MCA_orte_app_num': '0', 'NVIDIA_VISIBLE_DEVICES': 'all', 'OMPI_MCA_btl_vader_single_copy_mechanism': 'none', 'SAGEMAKER_JOB_NAME': 'ptddp-mnist-test-2022-07-01-18-05-04-221', 'SM_CHANNELS': '[]', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'OMPI_MCA_orte_precondition_transports': '0640943619e0ff47-51ad4c4750a63c17', 'PYTHONUNBUFFERED': '1', 'OMPI_MCA_shmem_RUNTIME_QUERY_hint': 'mmap', 'OMPI_UNIVERSE_SIZE': '1', 'OMPI_MCA_rmaps_base_oversubscribe': '1', 'PMIX_MCA_mca_base_component_show_load_errors': '1', 'SM_HOSTS': '[\"algo-1\"]', 'LOCAL_RANK': '7', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-44661a7fb67e0f6fd65efc5a81fad538b526dfd3230e82f16e907e32fb8c62fb-customer', 'PMIX_HOSTNAME': 'ip-10-0-216-15.us-west-2.compute.internal', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'HOSTNAME': 'ip-10-0-216-15.us-west-2.compute.internal', 'OMPI_COMM_WORLD_NODE_RANK': '7', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'OMPI_MCA_ess_base_vpid': '7', 'SM_MODULE_NAME': 'train_ptddp_mnist', 'PMIX_SECURITY_MODE': 'native', 'MASTER_PORT': '7777', 'OMPI_MCA_orte_hnp_uri': '1841889280.0;tcp://10.0.216.15:47141', 'NVIDIA_REQUIRE_CUDA': 'cuda>=11.3 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=440,driver<441 driver>=450', 'OMPI_MCA_plm_rsh_num_concurrent': '1', 'MANUAL_BUILD': '0', 'NV_ML_REPO_ENABLED': '1', 'BRANCH_OFI': '1.2.0-aws', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'OMPI_MCA_btl': '^openib', 'OMPI_COMM_WORLD_LOCAL_RANK': '7', 'TORCH_CUDA_ARCH_LIST': '3.7 5.0 7.0+PTX 8.0', 'NCCL_VERSION': '2.10.3', 'NCCL_SOCKET_IFNAME': 'eth0', 'HFI_NO_BACKTRACE': '1', 'AWS_REGION': 'us-west-2', 'PMIX_GDS_MODULE': 'ds21,ds12,hash', 'SMDATAPARALLEL_WORLD_RANK': '7', 'PWD': '/opt/ml/code', 'OMPI_MCA_orte_abort_on_non_zero_status': '1', 'OMPI_MCA_orte_local_daemon_uri': '1841889280.0;tcp://10.0.216.15:47141', 'RDMAV_FORK_SAFE': '1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video', 'OMPI_MCA_oob_tcp_if_include': 'eth0', 'NCCL_DEBUG': 'INFO', 'OMPI_MCA_pml': 'ob1', 'HOROVOD_VERSION': '0.24.3', 'PMIX_SYSTEM_TMPDIR': '/tmp', 'WORLD_SIZE': '8', 'SM_USER_ENTRY_POINT': 'train_ptddp_mnist.py', 'OPEN_MPI_PATH': '/opt/amazon/openmpi', 'OMPI_ARGV': '/opt/conda/bin/python3.8 -m mpi4py train_ptddp_mnist.py', 'PMIX_BFROP_BUFFER_TYPE': 'PMIX_BFROP_BUFFER_NON_DESC', 'LD_PRELOAD': '/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so', 'NV_CUDA_CUDART_VERSION': '11.3.109-1', 'PMIX_PTL_MODULE': 'tcp,usock', 'HOME': '/root', 'FI_PROVIDER': 'efa', 'OMPI_NUM_APP_CTX': '1', 'LANG': 'C.UTF-8', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'PMIX_DSTORE_21_BASE_PATH': '/tmp/ompi.ip-10-0-216-15.0/pid.24/pmix_dstor_ds21_24', 'OMPI_MCA_orte_num_nodes': '1', 'PMIX_VERSION': '3.2.3', 'SMDATAPARALLEL_LOCAL_RANK': '7', 'CUDA_VERSION': '11.3.1', 'DMLC_INTERFACE': 'eth0', 'OMPI_MCA_orte_tag_output': '1', 'SM_NUM_CPUS': '96', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'DGLBACKEND': 'pytorch', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'USE_SMDEBUG': '0', 'OMPI_MCA_orte_tmpdir_base': '/tmp', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'PMIX_RANK': '7', 'PMIX_SERVER_URI2': '1841889280.0;tcp4://127.0.0.1:50901', 'PMIX_SERVER_URI3': '1841889280.0;tcp4://127.0.0.1:50901', 'SM_CURRENT_HOST': 'algo-1', 'OMPI_MCA_initial_wdir': '/opt/ml/code', 'OMPI_FIRST_RANKS': '0', 'OMPI_MCA_mpi_oversubscribe': '1', 'SMDATAPARALLEL_WORLD_SIZE': '8', 'OMPI_MCA_orte_jobfam_session_dir': '/tmp/ompi.ip-10-0-216-15.0/pid.24', 'MASTER_ADDR': 'algo-1', 'SM_INPUT_DATA_CONFIG': '{}', 'SM_NUM_GPUS': '8', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'PYTHONPA[1,mpirank:0,algo-1]<stdout>:TH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220430-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg', 'PMIX_SERVER_URI21': '1841889280.0;tcp4://127.0.0.1:50901', 'NV_ML_REPO_URL': 'https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64', 'SM_LOG_LEVEL': '20', 'PMIX_DSTORE_ESH_BASE_PATH': '/tmp/ompi.ip-10-0-216-15.0/pid.24/pmix_dstor_ds12_24', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_pytorch_ddp_enabled\":true},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"ptddp-mnist-test-2022-07-01-18-05-04-221\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-07-01-18-05-04-221/source/sourcedir.tar.gz\",\"module_name\":\"train_ptddp_mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_ptddp_mnist.py\"}', 'OMPI_MCA_orte_ess_num_procs': '8', 'OMPI_FILE_LOCATION': '/tmp/ompi.ip-10-0-216-15.0/pid.24/0/0', 'PYTHONIOENCODING': 'UTF-8', 'OMPI_MCA_plm_rsh_no_tree_spawn': '1', 'OMPI_COMM_WORLD_SIZE': '8', 'OMPI_MCA_orte_ess_node_rank': '0', 'SHLVL': '4', 'SAGEMAKER_REGION': 'us-west-2', 'SMDATAPARALLEL_SERVER_PORT': '7592', 'NVARCH': 'x86_64', 'PMIX_ID': '1841889281.0', 'CUDNN_VERSION': '8.2.0.53', 'SMDATAPARALLEL_SERVER_ADDR': 'localhost', 'OMPI_COMM_WORLD_LOCAL_SIZE': '8', 'EFA_VERSION': '1.15.1', 'PYTHONDONTWRITEBYTECODE': '1', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-11-3', 'PMIX_SERVER_TMPDIR': '/tmp/ompi.ip-10-0-216-15.0/pid.24', 'LD_LIBRARY_PATH': '/opt/amazon/openmpi/lib:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'NCCL_IB_DISABLE': '1', 'OMPI_COMMAND': 'smddprun', 'RANK': '0', 'SMDATAPARALLEL_USE_SINGLENODE': '1', 'OMPI_MCA_ess': '^singleton', 'OMPI_VERSION': '4.1.1', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_pytorch_ddp_enabled\":true}', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-07-01-18-05-04-221/source/sourcedir.tar.gz', 'PMIX_NAMESPACE': '1841889281', 'TRAINING_JOB_NAME': 'ptddp-mnist-test-2022-07-01-18-05-04-221', 'CURRENT_HOST': 'algo-1', 'LC_ALL': 'C.UTF-8', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:570106654206:training-job/ptddp-mnist-test-2022-07-01-18-05-04-221', 'PATH': '/opt/amazon/openmpi/bin:/opt/amazon/openmpi/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'SM_USER_ARGS': '[]', 'OMPI_MCA_pmix': '^s1,s2,cray,isolated', 'OMPI_MCA_ess_base_jobid': '1841889281', 'OMPI_MCA_btl_tcp_if_include': 'eth0', 'SM_HPS': '{}', 'OMPI_APP_CTX_NUM_PROCS': '8', 'IPATH_NO_BACKTRACE': '1', 'OMPI_MCA_orte_top_session_dir': '/tmp/ompi.ip-10-0-216-15.0', 'OMPI_MCA_orte_launch': '1', 'SMDATAPARALLEL_NUM_CONN': '5', 'SM_MODEL_DIR': '/opt/ml/model', 'DLC_CONTAINER_T\u001b[0m\n",
      "\u001b[34mYPE': 'training', 'OMPI_COMM_WORLD_RANK': '0', '_': '/opt/conda/bin/python3.8'})\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:environ({'OMPI_MCA_orte_app_num': '0', 'NVIDIA_VISIBLE_DEVICES': 'all', 'OMPI_MCA_btl_vader_single_copy_mechanism': 'none', 'SAGEMAKER_JOB_NAME': 'ptddp-mnist-test-2022-07-01-18-05-04-221', 'SM_CHANNELS': '[]', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'OMPI_MCA_orte_precondition_transports': '0640943619e0ff47-51ad4c4750a63c17', 'PYTHONUNBUFFERED': '1', 'OMPI_MCA_shmem_RUNTIME_QUERY_hint': 'mmap', 'OMPI_UNIVERSE_SIZE': '1', 'OMPI_MCA_rmaps_base_oversubscribe': '1', 'PMIX_MCA_mca_base_component_show_load_errors': '1', 'SM_HOSTS': '[\"algo-1\"]', 'LOCAL_RANK': '2', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-44661a7fb67e0f6fd65efc5a81fad538b526dfd3230e82f16e907e32fb8c62fb-customer', 'PMIX_HOSTNAME': 'ip-10-0-216-15.us-west-2.compute.internal', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'HOSTNAME': 'ip-10-0-216-15.us-west-2.compute.internal', 'OMPI_COMM_WORLD_NODE_RANK': '2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'OMPI_MCA_ess_base_vpid': '2', 'SM_MODULE_NAME': 'train_ptddp_mnist', 'PMIX_SECURITY_MODE': 'native', 'MASTER_PORT': '7777', 'OMPI_MCA_orte_hnp_uri': '1841889280.0;tcp://10.0.216.15:47141', 'NVIDIA_REQUIRE_CUDA': 'cuda>=11.3 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=440,driver<441 driver>=450', 'OMPI_MCA_plm_rsh_num_concurrent': '1', 'MANUAL_BUILD': '0', 'NV_ML_REPO_ENABLED': '1', 'BRANCH_OFI': '1.2.0-aws', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'OMPI_MCA_btl': '^openib', 'OMPI_COMM_WORLD_LOCAL_RANK': '2', 'TORCH_CUDA_ARCH_LIST': '3.7 5.0 7.0+PTX 8.0', 'NCCL_VERSION': '2.10.3', 'NCCL_SOCKET_IFNAME': 'eth0', 'HFI_NO_BACKTRACE': '1', 'AWS_REGION': 'us-west-2', 'PMIX_GDS_MODULE': 'ds21,ds12,hash', 'SMDATAPARALLEL_WORLD_RANK': '2', 'PWD': '/opt/ml/code', 'OMPI_MCA_orte_abort_on_non_zero_status': '1', 'OMPI_MCA_orte_local_daemon_uri': '1841889280.0;tcp://10.0.216.15:47141', 'RDMAV_FORK_SAFE': '1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video', 'OMPI_MCA_oob_tcp_if_include': 'eth0', 'NCCL_DEBUG': 'INFO', 'OMPI_MCA_pml': 'ob1', 'HOROVOD_VERSION': '0.24.3', 'PMIX_SYSTEM_TMPDIR': '/tmp', 'WORLD_SIZE': '8', 'SM_USER_ENTRY_POINT': 'train_ptddp_mnist.py', 'OPEN_MPI_PATH': '/opt/amazon/openmpi', 'OMPI_ARGV': '/opt/conda/bin/python3.8 -m mpi4py train_ptddp_mnist.py', 'PMIX_BFROP_BUFFER_TYPE': 'PMIX_BFROP_BUFFER_NON_DESC', 'LD_PRELOAD': '/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so', 'NV_CUDA_CUDART_VERSION': '11.3.109-1', 'PMIX_PTL_MODULE': 'tcp,usock', 'HOME': '/root', 'FI_PROVIDER': 'efa', 'OMPI_NUM_APP_CTX': '1', 'LANG': 'C.UTF-8', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'PMIX_DSTORE_21_BASE_PATH': '/tmp/ompi.ip-10-0-216-15.0/pid.24/pmix_dstor_ds21_24', 'OMPI_MCA_orte_num_nodes': '1', 'PMIX_VERSION': '3.2.3', 'SMDATAPARALLEL_LOCAL_RANK': '2', 'CUDA_VERSION': '11.3.1', 'DMLC_INTERFACE': 'eth0', 'OMPI_MCA_orte_tag_output': '1', 'SM_NUM_CPUS': '96', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'DGLBACKEND': 'pytorch', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'USE_SMDEBUG': '0', 'OMPI_MCA_orte_tmpdir_base': '/tmp', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'PMIX_RANK': '2', 'PMIX_SERVER_URI2': '1841889280.0;tcp4://127.0.0.1:50901', 'PMIX_SERVER_URI3': '1841889280.0;tcp4://127.0.0.1:50901', 'SM_CURRENT_HOST': 'algo-1', 'OMPI_MCA_initial_wdir': '/opt/ml/code', 'OMPI_FIRST_RANKS': '0', 'OMPI_MCA_mpi_oversubscribe': '1', 'SMDATAPARALLEL_WORLD_SIZE': '8', 'OMPI_MCA_orte_jobfam_session_dir': '/tmp/ompi.ip-10-0-216-15.0/pid.24', 'MASTER_ADDR': 'algo-1', 'SM_INPUT_DATA_CONFIG': '{}', 'SM_NUM_GPUS': '8', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'PYTHONPA[1,mpirank:1,algo-1]<stdout>:TH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220430-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg', 'PMIX_SERVER_URI21': '1841889280.0;tcp4://127.0.0.1:50901', 'NV_ML_REPO_URL': 'https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64', 'SM_LOG_LEVEL': '20', 'PMIX_DSTORE_ESH_BASE_PATH': '/tmp/ompi.ip-10-0-216-15.0/pid.24/pmix_dstor_ds12_24', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_pytorch_ddp_enabled\":true},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"ptddp-mnist-test-2022-07-01-18-05-04-221\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-07-01-18-05-04-221/source/sourcedir.tar.gz\",\"module_name\":\"train_ptddp_mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_ptddp_mnist.py\"}', 'OMPI_MCA_orte_ess_num_procs': '8', 'OMPI_FILE_LOCATION': '/tmp/ompi.ip-10-0-216-15.0/pid.24/0/0', 'PYTHONIOENCODING': 'UTF-8', 'OMPI_MCA_plm_rsh_no_tree_spawn': '1', 'OMPI_COMM_WORLD_SIZE': '8', 'OMPI_MCA_orte_ess_node_rank': '1', 'SHLVL': '4', 'SAGEMAKER_REGION': 'us-west-2', 'SMDATAPARALLEL_SERVER_PORT': '7592', 'NVARCH': 'x86_64', 'PMIX_ID': '1841889281.1', 'CUDNN_VERSION': '8.2.0.53', 'SMDATAPARALLEL_SERVER_ADDR': 'localhost', 'OMPI_COMM_WORLD_LOCAL_SIZE': '8', 'EFA_VERSION': '1.15.1', 'PYTHONDONTWRITEBYTECODE': '1', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-11-3', 'PMIX_SERVER_TMPDIR': '/tmp/ompi.ip-10-0-216-15.0/pid.24', 'LD_LIBRARY_PATH': '/opt/amazon/openmpi/lib:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'NCCL_IB_DISABLE': '1', 'OMPI_COMMAND': 'smddprun', 'RANK': '1', 'SMDATAPARALLEL_USE_SINGLENODE': '1', 'OMPI_MCA_ess': '^singleton', 'OMPI_VERSION': '4.1.1', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_pytorch_ddp_enabled\":true}', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-07-01-18-05-04-221/source/sourcedir.tar.gz', 'PMIX_NAMESPACE': '1841889281', 'TRAINING_JOB_NAME': 'ptddp-mnist-test-2022-07-01-18-05-04-221', 'CURRENT_HOST': 'algo-1', 'LC_ALL': 'C.UTF-8', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:570106654206:training-job/ptddp-mnist-test-2022-07-01-18-05-04-221', 'PATH': '/opt/amazon/openmpi/bin:/opt/amazon/openmpi/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'SM_USER_ARGS': '[]', 'OMPI_MCA_pmix': '^s1,s2,cray,isolated', 'OMPI_MCA_ess_base_jobid': '1841889281', 'OMPI_MCA_btl_tcp_if_include': 'eth0', 'SM_HPS': '{}', 'OMPI_APP_CTX_NUM_PROCS': '8', 'IPATH_NO_BACKTRACE': '1', 'OMPI_MCA_orte_top_session_dir': '/tmp/ompi.ip-10-0-216-15.0', 'OMPI_MCA_orte_launch': '1', 'SMDATAPARALLEL_NUM_CONN': '5', 'SM_MODEL_DIR': '/opt/ml/model', 'DLC_CONTAINER_TYPE': 'training', 'OMPI_COMM_WORLD_RANK': '1', '_': '/opt/conda/bin/python3.8'})\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:TH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220430-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg', 'PMIX_SERVER_URI21': '1841889280.0;tcp4://127.0.0.1:50901', 'NV_ML_REPO_URL': 'https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64', 'SM_LOG_LEVEL': '20', 'PMIX_DSTORE_ESH_BASE_PATH': '/tmp/ompi.ip-10-0-216-15.0/pid.24/pmix_dstor_ds12_24', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_pytorch_ddp_enabled\":true},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"ptddp-mnist-test-2022-07-01-18-05-04-221\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-07-01-18-05-04-221/source/sourcedir.tar.gz\",\"module_name\":\"train_ptddp_mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_ptddp_mnist.py\"}', 'OMPI_MCA_orte_ess_num_procs': '8', 'OMPI_FILE_LOCATION': '/tmp/ompi.ip-10-0-216-15.0/pid.24/0/0', 'PYTHONIOENCODING': 'UTF-8', 'OMPI_MCA_plm_rsh_no_tree_spawn': '1', 'OMPI_COMM_WORLD_SIZE': '8', 'OMPI_MCA_orte_ess_node_rank': '3', 'SHLVL': '4', 'SAGEMAKER_REGION': 'us-west-2', 'SMDATAPARALLEL_SERVER_PORT': '7592', 'NVARCH': 'x86_64', 'PMIX_ID': '1841889281.3', 'CUDNN_VERSION': '8.2.0.53', 'SMDATAPARALLEL_SERVER_ADDR': 'localhost', 'OMPI_COMM_WORLD_LOCAL_SIZE': '8', 'EFA_VERSION': '1.15.1', 'PYTHONDONTWRITEBYTECODE': '1', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-11-3', 'PMIX_SERVER_TMPDIR': '/tmp/ompi.ip-10-0-216-15.0/pid.24', 'LD_LIBRARY_PATH': '/opt/amazon/openmpi/lib:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'NCCL_IB_DISABLE': '1', 'OMPI_COMMAND': 'smddprun', 'RANK': '3', 'SMDATAPARALLEL_USE_SINGLENODE': '1', 'OMPI_MCA_ess': '^singleton', 'OMPI_VERSION': '4.1.1', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_pytorch_ddp_enabled\":true}', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-07-01-18-05-04-221/source/sourcedir.tar.gz', 'PMIX_NAMESPACE': '1841889281', 'TRAINING_JOB_NAME': 'ptddp-mnist-test-2022-07-01-18-05-04-221', 'CURRENT_HOST': 'algo-1', 'LC_ALL': 'C.UTF-8', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:570106654206:training-job/ptddp-mnist-test-2022-07-01-18-05-04-221', 'PATH': '/opt/amazon/openmpi/bin:/opt/amazon/openmpi/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'SM_USER_ARGS': '[]', 'OMPI_MCA_pmix': '^s1,s2,cray,isolated', 'OMPI_MCA_ess_base_jobid': '1841889281', 'OMPI_MCA_btl_tcp_if_include': 'eth0', 'SM_HPS': '{}', 'OMPI_APP_CTX_NUM_PROCS': '8', 'IPATH_NO_BACKTRACE': '1', 'OMPI_MCA_orte_top_session_dir': '/tmp/ompi.ip-10-0-216-15.0', 'OMPI_MCA_orte_launch': '1', 'SMDATAPARALLEL_NUM_CONN': '5', 'SM_MODEL_DIR': '/opt/ml/model', 'DLC_CONTAINER_TYPE': 'training', 'OMPI_COMM_WORLD_RANK': '3', '_': '/opt/conda/bin/python3.8'})\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:TH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220430-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg', 'PMIX_SERVER_URI21': '1841889280.0;tcp4://127.0.0.1:50901', 'NV_ML_REPO_URL': 'https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64', 'SM_LOG_LEVEL': '20', 'PMIX_DSTORE_ESH_BASE_PATH': '/tmp/ompi.ip-10-0-216-15.0/pid.24/pmix_dstor_ds12_24', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_pytorch_ddp_enabled\":true},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"ptddp-mnist-test-2022-07-01-18-05-04-221\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-07-01-18-05-04-221/source/sourcedir.tar.gz\",\"module_name\":\"train_ptddp_mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_ptddp_mnist.py\"}', 'OMPI_MCA_orte_ess_num_procs': '8', 'OMPI_FILE_LOCATION': '/tmp/ompi.ip-10-0-216-15.0/pid.24/0/0', 'PYTHONIOENCODING': 'UTF-8', 'OMPI_MCA_plm_rsh_no_tree_spawn': '1', 'OMPI_COMM_WORLD_SIZE': '8', 'OMPI_MCA_orte_ess_node_rank': '4', 'SHLVL': '4', 'SAGEMAKER_REGION': 'us-west-2', 'SMDATAPARALLEL_SERVER_PORT': '7592', 'NVARCH': 'x86_64', 'PMIX_ID': '1841889281.4', 'CUDNN_VERSION': '8.2.0.53', 'SMDATAPARALLEL_SERVER_ADDR': 'localhost', 'OMPI_COMM_WORLD_LOCAL_SIZE': '8', 'EFA_VERSION': '1.15.1', 'PYTHONDONTWRITEBYTECODE': '1', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-11-3', 'PMIX_SERVER_TMPDIR': '/tmp/ompi.ip-10-0-216-15.0/pid.24', 'LD_LIBRARY_PATH': '/opt/amazon/openmpi/lib:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'NCCL_IB_DISABLE': '1', 'OMPI_COMMAND': 'smddprun', 'RANK': '4', 'SMDATAPARALLEL_USE_SINGLENODE': '1', 'OMPI_MCA_ess': '^singleton', 'OMPI_VERSION': '4.1.1', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_pytorch_ddp_enabled\":true}', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-07-01-18-05-04-221/source/sourcedir.tar.gz', 'PMIX_NAMESPACE': '1841889281', 'TRAINING_JOB_NAME': 'ptddp-mnist-test-2022-07-01-18-05-04-221', 'CURRENT_HOST': 'algo-1', 'LC_ALL': 'C.UTF-8', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:570106654206:training-job/ptddp-mnist-test-2022-07-01-18-05-04-221', 'PATH': '/opt/amazon/openmpi/bin:/opt/amazon/openmpi/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'SM_USER_ARGS': '[]', 'OMPI_MCA_pmix': '^s1,s2,cray,isolated', 'OMPI_MCA_ess_base_jobid': '1841889281', 'OMPI_MCA_btl_tcp_if_include': 'eth0', 'SM_HPS': '{}', 'OMPI_APP_CTX_NUM_PROCS': '8', 'IPATH_NO_BACKTRACE': '1', 'OMPI_MCA_orte_top_session_dir': '/tmp/ompi.ip-10-0-216-15.0', 'OMPI_MCA_orte_launch': '1', 'SMDATAPARALLEL_NUM_CONN': '5', 'SM_MODEL_DIR': '/opt/ml/model', 'DLC_CONTAINER_TYPE': 'training', 'OMPI_COMM_WORLD_RANK': '4', '_': '/opt/conda/bin/python3.8'})\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:TH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220430-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg', 'PMIX_SERVER_URI21': '1841889280.0;tcp4://127.0.0.1:50901', 'NV_ML_REPO_URL': 'https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64', 'SM_LOG_LEVEL': '20', 'PMIX_DSTORE_ESH_BASE_PATH': '/tmp/ompi.ip-10-0-216-15.0/pid.24/pmix_dstor_ds12_24', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_pytorch_ddp_enabled\":true},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"ptddp-mnist-test-2022-07-01-18-05-04-221\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-07-01-18-05-04-221/source/sourcedir.tar.gz\",\"module_name\":\"train_ptddp_mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_ptddp_mnist.py\"}', 'OMPI_MCA_orte_ess_num_procs': '8', 'OMPI_FILE_LOCATION': '/tmp/ompi.ip-10-0-216-15.0/pid.24/0/0', 'PYTHONIOENCODING': 'UTF-8', 'OMPI_MCA_plm_rsh_no_tree_spawn': '1', 'OMPI_COMM_WORLD_SIZE': '8', 'OMPI_MCA_orte_ess_node_rank': '5', 'SHLVL': '4', 'SAGEMAKER_REGION': 'us-west-2', 'SMDATAPARALLEL_SERVER_PORT': '7592', 'NVARCH': 'x86_64', 'PMIX_ID': '1841889281.5', 'CUDNN_VERSION': '8.2.0.53', 'SMDATAPARALLEL_SERVER_ADDR': 'localhost', 'OMPI_COMM_WORLD_LOCAL_SIZE': '8', 'EFA_VERSION': '1.15.1', 'PYTHONDONTWRITEBYTECODE': '1', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-11-3', 'PMIX_SERVER_TMPDIR': '/tmp/ompi.ip-10-0-216-15.0/pid.24', 'LD_LIBRARY_PATH': '/opt/amazon/openmpi/lib:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'NCCL_IB_DISABLE': '1', 'OMPI_COMMAND': 'smddprun', 'RANK': '5', 'SMDATAPARALLEL_USE_SINGLENODE': '1', 'OMPI_MCA_ess': '^singleton', 'OMPI_VERSION': '4.1.1', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_pytorch_ddp_enabled\":true}', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-07-01-18-05-04-221/source/sourcedir.tar.gz', 'PMIX_NAMESPACE': '1841889281', 'TRAINING_JOB_NAME': 'ptddp-mnist-test-2022-07-01-18-05-04-221', 'CURRENT_HOST': 'algo-1', 'LC_ALL': 'C.UTF-8', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:570106654206:training-job/ptddp-mnist-test-2022-07-01-18-05-04-221', 'PATH': '/opt/amazon/openmpi/bin:/opt/amazon/openmpi/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'SM_USER_ARGS': '[]', 'OMPI_MCA_pmix': '^s1,s2,cray,isolated', 'OMPI_MCA_ess_base_jobid': '1841889281', 'OMPI_MCA_btl_tcp_if_include': 'eth0', 'SM_HPS': '{}', 'OMPI_APP_CTX_NUM_PROCS': '8', 'IPATH_NO_BACKTRACE': '1', 'OMPI_MCA_orte_top_session_dir': '/tmp/ompi.ip-10-0-216-15.0', 'OMPI_MCA_orte_launch': '1', 'SMDATAPARALLEL_NUM_CONN': '5', 'SM_MODEL_DIR': '/opt/ml/model', 'DLC_CONTAINER_TYPE': 'training', 'OMPI_COMM_WORLD_RANK': '5', '_': '/opt/conda/bin/python3.8'})\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:TH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220430-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg', 'PMIX_SERVER_URI21': '1841889280.0;tcp4://127.0.0.1:50901', 'NV_ML_REPO_URL': 'https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64', 'SM_LOG_LEVEL': '20', 'PMIX_DSTORE_ESH_BASE_PATH': '/tmp/ompi.ip-10-0-216-15.0/pid.24/pmix_dstor_ds12_24', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_pytorch_ddp_enabled\":true},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"ptddp-mnist-test-2022-07-01-18-05-04-221\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-07-01-18-05-04-221/source/sourcedir.tar.gz\",\"module_name\":\"train_ptddp_mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_ptddp_mnist.py\"}', 'OMPI_MCA_orte_ess_num_procs': '8', 'OMPI_FILE_LOCATION': '/tmp/ompi.ip-10-0-216-15.0/pid.24/0/0', 'PYTHONIOENCODING': 'UTF-8', 'OMPI_MCA_plm_rsh_no_tree_spawn': '1', 'OMPI_COMM_WORLD_SIZE': '8', 'OMPI_MCA_orte_ess_node_rank': '6', 'SHLVL': '4', 'SAGEMAKER_REGION': 'us-west-2', 'SMDATAPARALLEL_SERVER_PORT': '7592', 'NVARCH': 'x86_64', 'PMIX_ID': '1841889281.6', 'CUDNN_VERSION': '8.2.0.53', 'SMDATAPARALLEL_SERVER_ADDR': 'localhost', 'OMPI_COMM_WORLD_LOCAL_SIZE': '8', 'EFA_VERSION': '1.15.1', 'PYTHONDONTWRITEBYTECODE': '1', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-11-3', 'PMIX_SERVER_TMPDIR': '/tmp/ompi.ip-10-0-216-15.0/pid.24', 'LD_LIBRARY_PATH': '/opt/amazon/openmpi/lib:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'NCCL_IB_DISABLE': '1', 'OMPI_COMMAND': 'smddprun', 'RANK': '6', 'SMDATAPARALLEL_USE_SINGLENODE': '1', 'OMPI_MCA_ess': '^singleton', 'OMPI_VERSION': '4.1.1', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_pytorch_ddp_enabled\":true}', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-07-01-18-05-04-221/source/sourcedir.tar.gz', 'PMIX_NAMESPACE': '1841889281', 'TRAINING_JOB_NAME': 'ptddp-mnist-test-2022-07-01-18-05-04-221', 'CURRENT_HOST': 'algo-1', 'LC_ALL': 'C.UTF-8', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:570106654206:training-job/ptddp-mnist-test-2022-07-01-18-05-04-221', 'PATH': '/opt/amazon/openmpi/bin:/opt/amazon/openmpi/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'SM_USER_ARGS': '[]', 'OMPI_MCA_pmix': '^s1,s2,cray,isolated', 'OMPI_MCA_ess_base_jobid': '1841889281', 'OMPI_MCA_btl_tcp_if_include': 'eth0', 'SM_HPS': '{}', 'OMPI_APP_CTX_NUM_PROCS': '8', 'IPATH_NO_BACKTRACE': '1', 'OMPI_MCA_orte_top_session_dir': '/tmp/ompi.ip-10-0-216-15.0', 'OMPI_MCA_orte_launch': '1', 'SMDATAPARALLEL_NUM_CONN': '5', 'SM_MODEL_DIR': '/opt/ml/model', 'DLC_CONTAINER_TYPE': 'training', 'OMPI_COMM_WORLD_RANK': '6', '_': '/opt/conda/bin/python3.8'})\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:TH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220430-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg', 'PMIX_SERVER_URI21': '1841889280.0;tcp4://127.0.0.1:50901', 'NV_ML_REPO_URL': 'https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64', 'SM_LOG_LEVEL': '20', 'PMIX_DSTORE_ESH_BASE_PATH': '/tmp/ompi.ip-10-0-216-15.0/pid.24/pmix_dstor_ds12_24', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_pytorch_ddp_enabled\":true},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"ptddp-mnist-test-2022-07-01-18-05-04-221\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-07-01-18-05-04-221/source/sourcedir.tar.gz\",\"module_name\":\"train_ptddp_mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_ptddp_mnist.py\"}', 'OMPI_MCA_orte_ess_num_procs': '8', 'OMPI_FILE_LOCATION': '/tmp/ompi.ip-10-0-216-15.0/pid.24/0/0', 'PYTHONIOENCODING': 'UTF-8', 'OMPI_MCA_plm_rsh_no_tree_spawn': '1', 'OMPI_COMM_WORLD_SIZE': '8', 'OMPI_MCA_orte_ess_node_rank': '7', 'SHLVL': '4', 'SAGEMAKER_REGION': 'us-west-2', 'SMDATAPARALLEL_SERVER_PORT': '7592', 'NVARCH': 'x86_64', 'PMIX_ID': '1841889281.7', 'CUDNN_VERSION': '8.2.0.53', 'SMDATAPARALLEL_SERVER_ADDR': 'localhost', 'OMPI_COMM_WORLD_LOCAL_SIZE': '8', 'EFA_VERSION': '1.15.1', 'PYTHONDONTWRITEBYTECODE': '1', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-11-3', 'PMIX_SERVER_TMPDIR': '/tmp/ompi.ip-10-0-216-15.0/pid.24', 'LD_LIBRARY_PATH': '/opt/amazon/openmpi/lib:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'NCCL_IB_DISABLE': '1', 'OMPI_COMMAND': 'smddprun', 'RANK': '7', 'SMDATAPARALLEL_USE_SINGLENODE': '1', 'OMPI_MCA_ess': '^singleton', 'OMPI_VERSION': '4.1.1', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_pytorch_ddp_enabled\":true}', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-07-01-18-05-04-221/source/sourcedir.tar.gz', 'PMIX_NAMESPACE': '1841889281', 'TRAINING_JOB_NAME': 'ptddp-mnist-test-2022-07-01-18-05-04-221', 'CURRENT_HOST': 'algo-1', 'LC_ALL': 'C.UTF-8', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:570106654206:training-job/ptddp-mnist-test-2022-07-01-18-05-04-221', 'PATH': '/opt/amazon/openmpi/bin:/opt/amazon/openmpi/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'SM_USER_ARGS': '[]', 'OMPI_MCA_pmix': '^s1,s2,cray,isolated', 'OMPI_MCA_ess_base_jobid': '1841889281', 'OMPI_MCA_btl_tcp_if_include': 'eth0', 'SM_HPS': '{}', 'OMPI_APP_CTX_NUM_PROCS': '8', 'IPATH_NO_BACKTRACE': '1', 'OMPI_MCA_orte_top_session_dir': '/tmp/ompi.ip-10-0-216-15.0', 'OMPI_MCA_orte_launch': '1', 'SMDATAPARALLEL_NUM_CONN': '5', 'SM_MODEL_DIR': '/opt/ml/model', 'DLC_CONTAINER_TYPE': 'training', 'OMPI_COMM_WORLD_RANK': '7', '_': '/opt/conda/bin/python3.8'})\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:TH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220430-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg', 'PMIX_SERVER_URI21': '1841889280.0;tcp4://127.0.0.1:50901', 'NV_ML_REPO_URL': 'https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64', 'SM_LOG_LEVEL': '20', 'PMIX_DSTORE_ESH_BASE_PATH': '/tmp/ompi.ip-10-0-216-15.0/pid.24/pmix_dstor_ds12_24', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_pytorch_ddp_enabled\":true},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"ptddp-mnist-test-2022-07-01-18-05-04-221\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-07-01-18-05-04-221/source/sourcedir.tar.gz\",\"module_name\":\"train_ptddp_mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_ptddp_mnist.py\"}', 'OMPI_MCA_orte_ess_num_procs': '8', 'OMPI_FILE_LOCATION': '/tmp/ompi.ip-10-0-216-15.0/pid.24/0/0', 'PYTHONIOENCODING': 'UTF-8', 'OMPI_MCA_plm_rsh_no_tree_spawn': '1', 'OMPI_COMM_WORLD_SIZE': '8', 'OMPI_MCA_orte_ess_node_rank': '2', 'SHLVL': '4', 'SAGEMAKER_REGION': 'us-west-2', 'SMDATAPARALLEL_SERVER_PORT': '7592', 'NVARCH': 'x86_64', 'PMIX_ID': '1841889281.2', 'CUDNN_VERSION': '8.2.0.53', 'SMDATAPARALLEL_SERVER_ADDR': 'localhost', 'OMPI_COMM_WORLD_LOCAL_SIZE': '8', 'EFA_VERSION': '1.15.1', 'PYTHONDONTWRITEBYTECODE': '1', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-11-3', 'PMIX_SERVER_TMPDIR': '/tmp/ompi.ip-10-0-216-15.0/pid.24', 'LD_LIBRARY_PATH': '/opt/amazon/openmpi/lib:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'NCCL_IB_DISABLE': '1', 'OMPI_COMMAND': 'smddprun', 'RANK': '2', 'SMDATAPARALLEL_USE_SINGLENODE': '1', 'OMPI_MCA_ess': '^singleton', 'OMPI_VERSION': '4.1.1', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_pytorch_ddp_enabled\":true}', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-07-01-18-05-04-221/source/sourcedir.tar.gz', 'PMIX_NAMESPACE': '1841889281', 'TRAINING_JOB_NAME': 'ptddp-mnist-test-2022-07-01-18-05-04-221', 'CURRENT_HOST': 'algo-1', 'LC_ALL': 'C.UTF-8', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:570106654206:training-job/ptddp-mnist-test-2022-07-01-18-05-04-221', 'PATH': '/opt/amazon/openmpi/bin:/opt/amazon/openmpi/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'SM_USER_ARGS': '[]', 'OMPI_MCA_pmix': '^s1,s2,cray,isolated', 'OMPI_MCA_ess_base_jobid': '1841889281', 'OMPI_MCA_btl_tcp_if_include': 'eth0', 'SM_HPS': '{}', 'OMPI_APP_CTX_NUM_PROCS': '8', 'IPATH_NO_BACKTRACE': '1', 'OMPI_MCA_orte_top_session_dir': '/tmp/ompi.ip-10-0-216-15.0', 'OMPI_MCA_orte_launch': '1', 'SMDATAPARALLEL_NUM_CONN': '5', 'SM_MODEL_DIR': '/opt/ml/model', 'DLC_CONTAINER_TYPE': 'training', 'OMPI_COMM_WORLD_RANK': '2', '_': '/opt/conda/bin/python3.8'})\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/9912422 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|         | 347136/9912422 [00:00<00:02, 3446824.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|       | 2856960/9912422 [00:00<00:00, 16113990.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%| | 8690688/9912422 [00:00<00:00, 34812057.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#0159913344it [00:00, 30163462.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Extracting /tmp/data/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/28881 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#01529696it [00:00, 1119798.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Extracting /tmp/data/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /tmp/data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/1648877 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|     | 830464/1648877 [00:00<00:00, 8250259.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#0151649664it [00:00, 12820156.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Extracting /tmp/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/4542 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#0155120it [00:00, 51252593.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Extracting /tmp/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:64 [0] NCCL INFO Bootstrap : Using eth0:10.0.216.15<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:64 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:64 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.2.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:64 [0] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:64 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:64 [0] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:64 [0] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:64 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:49 [5] NCCL INFO Bootstrap : Using eth0:10.0.216.15<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:51 [6] NCCL INFO Bootstrap : Using eth0:10.0.216.15<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:42 [2] NCCL INFO Bootstrap : Using eth0:10.0.216.15<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:52 [7] NCCL INFO Bootstrap : Using eth0:10.0.216.15<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:39 [1] NCCL INFO Bootstrap : Using eth0:10.0.216.15<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:45 [3] NCCL INFO Bootstrap : Using eth0:10.0.216.15<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:47 [4] NCCL INFO Bootstrap : Using eth0:10.0.216.15<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:49 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:49 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.2.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:51 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:51 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.2.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:42 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:42 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.2.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:52 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:52 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.2.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:45 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:45 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.2.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:47 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:49 [5] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:51 [6] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:39 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:39 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.2.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:45 [3] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:45 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:47 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.2.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:47 [4] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:49 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:42 [2] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:42 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:51 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:52 [7] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:52 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:39 [1] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:39 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:47 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:45 [3] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:45 [3] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:45 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:42 [2] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:42 [2] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:42 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:49 [5] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:49 [5] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:49 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:52 [7] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:52 [7] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:52 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:39 [1] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:39 [1] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:47 [4] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:51 [6] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:39 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:51 [6] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:47 [4] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:51 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:47 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 00 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 00 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 00 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 00 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 00 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 00 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 00 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 01 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 00 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 01 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 01 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 01 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 01 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 01 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 01 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 02 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 01 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 02 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 02 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 02 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 02 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 02 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 02 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 02 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 03 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 03 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 03 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 03 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 03 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 03 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 03 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 04 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 03 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 04 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 04 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 04 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 04 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 04 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 04 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 04 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 05 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 05 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 05 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 05 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 05 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 05 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 05 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 06 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 05 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 06 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 06 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 06 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 06 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 06 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 06 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 07 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 06 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 07 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 07 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 07 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 07 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 07 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 07 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 08 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 07 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 08 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 08 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 08 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 08 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 08 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 08 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 08 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 09 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 09 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 09 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 09 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 09 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 09 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 09 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 09 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 10 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 10 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 10 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 10 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 10 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 10 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 10 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 10 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 11 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 11 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 11 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 11 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 11 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 11 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 11 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 12 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 11 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 12 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 12 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 12 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 12 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 12 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 12 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 12 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 13 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 13 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 13 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 13 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 13 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 13 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 13 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 14 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 13 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 14 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 14 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 14 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 14 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 14 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 14 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 15 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 14 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 15 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 15 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 15 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 15 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 15 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 15 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 16 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 15 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 16 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 16 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 16 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 16 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 16 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 16 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 17 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 16 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 17 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 17 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 17 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 17 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 17 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 17 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 18 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 17 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 18 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 18 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 18 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 18 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 18 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 18 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 18 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 19 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 19 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 19 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 19 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 19 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 19 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 19 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 20 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 19 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 20 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 20 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 20 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 20 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 20 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 20 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 20 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 21 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 21 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 21 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 21 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 21 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 21 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 21 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 22 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 21 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 22 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 22 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 22 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 22 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 22 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 22 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 23 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 22 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 23 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 23 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 23 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 23 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Channel 23 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 23 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 23 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 00 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 01 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 02 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 03 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 04 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 05 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 06 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 07 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 08 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 09 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 10 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 11 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 12 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 13 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 14 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 15 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 16 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 17 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 18 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 19 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 20 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 21 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 22 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Channel 23 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 00 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 00 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 00 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 00 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 00 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 00 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 01 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 01 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 01 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 01 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 01 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 01 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 02 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 02 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 02 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 02 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 02 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 02 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 03 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 03 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 03 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 03 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 03 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 03 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 04 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 04 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 04 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 04 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 04 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 04 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 05 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 05 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 05 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 05 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 05 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 05 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 06 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 06 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 06 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 06 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 06 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 06 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 07 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 07 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 07 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 07 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 07 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 07 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 08 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 08 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 08 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 08 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 08 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 08 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 09 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 09 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 09 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 09 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 09 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 09 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 10 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 10 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 10 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 10 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 10 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 10 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 11 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 11 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 11 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 11 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 11 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 11 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 12 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 12 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 12 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 12 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 12 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 12 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 13 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 13 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 13 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 13 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 13 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 13 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 14 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 14 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 14 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 14 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 14 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 14 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 15 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 15 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 15 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 15 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 15 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 15 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 16 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 16 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 16 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 16 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 16 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 16 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 17 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 17 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 17 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 17 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 17 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 17 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 18 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 18 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 18 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 18 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 18 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 18 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 19 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 19 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 19 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 19 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 19 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 19 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 20 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 20 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 20 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 20 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 20 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 20 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 21 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 21 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 21 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 21 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 21 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 21 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 22 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 22 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 22 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 22 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 22 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 22 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Channel 23 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Channel 23 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Channel 23 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Channel 23 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Channel 23 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Channel 23 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:49:656 [5] NCCL INFO comm 0x7f1094002fb0 rank 5 nranks 8 cudaDev 5 busId 901d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:45:654 [3] NCCL INFO comm 0x7f8430002fb0 rank 3 nranks 8 cudaDev 3 busId 201d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:42:655 [2] NCCL INFO comm 0x7fa0d8002fb0 rank 2 nranks 8 cudaDev 2 busId 201c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:51:659 [6] NCCL INFO comm 0x7fc238002fb0 rank 6 nranks 8 cudaDev 6 busId a01c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:39:658 [1] NCCL INFO comm 0x7faf80002fb0 rank 1 nranks 8 cudaDev 1 busId 101d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:52:657 [7] NCCL INFO comm 0x7f6914002fb0 rank 7 nranks 8 cudaDev 7 busId a01d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:47:660 [4] NCCL INFO comm 0x7f89b0002fb0 rank 4 nranks 8 cudaDev 4 busId 901c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:653 [0] NCCL INFO comm 0x7f1be8002fb0 rank 0 nranks 8 cudaDev 0 busId 101c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:64:64 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1320: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1320: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1320: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1320: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1320: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1320: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1320: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1320: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [0/60000 (0%)]#011Loss: 2.305867\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:root:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:root:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:root:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:root:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:root:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:root:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:root:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:root:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [5120/60000 (8%)]#011Loss: 1.718301\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [10240/60000 (17%)]#011Loss: 1.392494\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [15360/60000 (25%)]#011Loss: 1.039629\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [20480/60000 (34%)]#011Loss: 1.604734\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [25600/60000 (42%)]#011Loss: 1.377143\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [30720/60000 (51%)]#011Loss: 1.304662\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [35840/60000 (59%)]#011Loss: 1.232839\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [40960/60000 (68%)]#011Loss: 1.182060\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [46080/60000 (76%)]#011Loss: 1.084365\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [51200/60000 (85%)]#011Loss: 1.374212\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [56320/60000 (93%)]#011Loss: 1.405413\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Average loss: 0.1554, Accuracy: 9729/10000 (97%)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [0/60000 (0%)]#011Loss: 1.089756\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [5120/60000 (8%)]#011Loss: 1.274374\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [10240/60000 (17%)]#011Loss: 1.023966\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [15360/60000 (25%)]#011Loss: 1.402654\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [20480/60000 (34%)]#011Loss: 1.188576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [25600/60000 (42%)]#011Loss: 1.154102\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [30720/60000 (51%)]#011Loss: 1.184004\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [35840/60000 (59%)]#011Loss: 1.129843\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [40960/60000 (68%)]#011Loss: 1.154431\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [46080/60000 (76%)]#011Loss: 1.118056\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [51200/60000 (85%)]#011Loss: 1.249419\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [56320/60000 (93%)]#011Loss: 1.191592\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Average loss: 0.1135, Accuracy: 9844/10000 (98%)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [0/60000 (0%)]#011Loss: 1.090895\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [5120/60000 (8%)]#011Loss: 1.010575\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [10240/60000 (17%)]#011Loss: 1.091680\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [15360/60000 (25%)]#011Loss: 1.235801\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [20480/60000 (34%)]#011Loss: 1.103448\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [25600/60000 (42%)]#011Loss: 0.978068\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [30720/60000 (51%)]#011Loss: 1.015875\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [35840/60000 (59%)]#011Loss: 1.237556\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [40960/60000 (68%)]#011Loss: 1.318992\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [46080/60000 (76%)]#011Loss: 1.300420\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [51200/60000 (85%)]#011Loss: 1.150264\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [56320/60000 (93%)]#011Loss: 1.075705\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Average loss: 0.1299, Accuracy: 9819/10000 (98%)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [0/60000 (0%)]#011Loss: 0.931632\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [5120/60000 (8%)]#011Loss: 0.972349\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [10240/60000 (17%)]#011Loss: 0.831533\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [15360/60000 (25%)]#011Loss: 0.944013\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [20480/60000 (34%)]#011Loss: 1.208546\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [25600/60000 (42%)]#011Loss: 0.822862\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [30720/60000 (51%)]#011Loss: 1.258399\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [35840/60000 (59%)]#011Loss: 1.217458\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [40960/60000 (68%)]#011Loss: 1.045056\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [46080/60000 (76%)]#011Loss: 1.243636\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [51200/60000 (85%)]#011Loss: 1.337571\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [56320/60000 (93%)]#011Loss: 1.484018\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Average loss: 0.0830, Accuracy: 9854/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [0/60000 (0%)]#011Loss: 1.246904\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [5120/60000 (8%)]#011Loss: 1.443795\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [10240/60000 (17%)]#011Loss: 1.343920\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [15360/60000 (25%)]#011Loss: 1.268316\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [20480/60000 (34%)]#011Loss: 1.225582\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [25600/60000 (42%)]#011Loss: 0.916311\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [30720/60000 (51%)]#011Loss: 1.006481\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [35840/60000 (59%)]#011Loss: 1.309333\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [40960/60000 (68%)]#011Loss: 0.955418\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [46080/60000 (76%)]#011Loss: 1.198615\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [51200/60000 (85%)]#011Loss: 1.030086\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [56320/60000 (93%)]#011Loss: 0.938529\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Average loss: 0.0698, Accuracy: 9883/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 6 [0/60000 (0%)]#011Loss: 1.437224\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 6 [5120/60000 (8%)]#011Loss: 1.050424\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 6 [10240/60000 (17%)]#011Loss: 1.054872\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 6 [15360/60000 (25%)]#011Loss: 1.136662\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 6 [20480/60000 (34%)]#011Loss: 1.236384\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 6 [25600/60000 (42%)]#011Loss: 1.005792\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 6 [30720/60000 (51%)]#011Loss: 1.173715\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 6 [35840/60000 (59%)]#011Loss: 0.859791\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 6 [40960/60000 (68%)]#011Loss: 1.199762\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 6 [46080/60000 (76%)]#011Loss: 1.406139\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 6 [51200/60000 (85%)]#011Loss: 1.122966\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 6 [56320/60000 (93%)]#011Loss: 1.318084\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Average loss: 0.0651, Accuracy: 9881/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 7 [0/60000 (0%)]#011Loss: 1.160901\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 7 [5120/60000 (8%)]#011Loss: 1.265794\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 7 [10240/60000 (17%)]#011Loss: 1.156904\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 7 [15360/60000 (25%)]#011Loss: 1.238326\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 7 [20480/60000 (34%)]#011Loss: 1.119519\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 7 [25600/60000 (42%)]#011Loss: 1.078644\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 7 [30720/60000 (51%)]#011Loss: 1.326459\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 7 [35840/60000 (59%)]#011Loss: 1.298396\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 7 [40960/60000 (68%)]#011Loss: 1.124671\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 7 [46080/60000 (76%)]#011Loss: 0.685941\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 7 [51200/60000 (85%)]#011Loss: 1.268181\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 7 [56320/60000 (93%)]#011Loss: 1.054335\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Average loss: 0.0613, Accuracy: 9886/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 8 [0/60000 (0%)]#011Loss: 0.900440\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 8 [5120/60000 (8%)]#011Loss: 1.261615\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 8 [10240/60000 (17%)]#011Loss: 1.082766\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 8 [15360/60000 (25%)]#011Loss: 1.337825\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 8 [20480/60000 (34%)]#011Loss: 1.139726\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 8 [25600/60000 (42%)]#011Loss: 1.534657\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 8 [30720/60000 (51%)]#011Loss: 1.084861\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 8 [35840/60000 (59%)]#011Loss: 1.051022\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 8 [40960/60000 (68%)]#011Loss: 1.054878\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 8 [46080/60000 (76%)]#011Loss: 1.259995\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 8 [51200/60000 (85%)]#011Loss: 1.392217\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 8 [56320/60000 (93%)]#011Loss: 1.232904\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Average loss: 0.0638, Accuracy: 9887/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 9 [0/60000 (0%)]#011Loss: 1.335510\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 9 [5120/60000 (8%)]#011Loss: 1.193204\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 9 [10240/60000 (17%)]#011Loss: 1.277730\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 9 [15360/60000 (25%)]#011Loss: 1.119456\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 9 [20480/60000 (34%)]#011Loss: 1.220477\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 9 [25600/60000 (42%)]#011Loss: 1.086686\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 9 [30720/60000 (51%)]#011Loss: 1.296228\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 9 [35840/60000 (59%)]#011Loss: 1.274881\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 9 [40960/60000 (68%)]#011Loss: 1.045424\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 9 [46080/60000 (76%)]#011Loss: 1.268194\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 9 [51200/60000 (85%)]#011Loss: 1.450980\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 9 [56320/60000 (93%)]#011Loss: 1.183077\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Average loss: 0.0609, Accuracy: 9884/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 10 [0/60000 (0%)]#011Loss: 1.198694\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 10 [5120/60000 (8%)]#011Loss: 1.133143\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 10 [10240/60000 (17%)]#011Loss: 1.270481\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 10 [15360/60000 (25%)]#011Loss: 1.439488\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 10 [20480/60000 (34%)]#011Loss: 1.044441\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 10 [25600/60000 (42%)]#011Loss: 1.234737\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 10 [30720/60000 (51%)]#011Loss: 1.108391\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 10 [35840/60000 (59%)]#011Loss: 1.029411\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 10 [40960/60000 (68%)]#011Loss: 1.370760\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 10 [46080/60000 (76%)]#011Loss: 1.250510\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 10 [51200/60000 (85%)]#011Loss: 1.189517\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 10 [56320/60000 (93%)]#011Loss: 1.014771\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Average loss: 0.0591, Accuracy: 9881/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 11 [0/60000 (0%)]#011Loss: 1.159432\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 11 [5120/60000 (8%)]#011Loss: 1.003848\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 11 [10240/60000 (17%)]#011Loss: 1.086586\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 11 [15360/60000 (25%)]#011Loss: 0.971809\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 11 [20480/60000 (34%)]#011Loss: 1.056493\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 11 [25600/60000 (42%)]#011Loss: 1.398301\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 11 [30720/60000 (51%)]#011Loss: 1.104753\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 11 [35840/60000 (59%)]#011Loss: 1.439665\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 11 [40960/60000 (68%)]#011Loss: 1.016348\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 11 [46080/60000 (76%)]#011Loss: 0.968576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 11 [51200/60000 (85%)]#011Loss: 1.413766\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 11 [56320/60000 (93%)]#011Loss: 1.050200\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Average loss: 0.0582, Accuracy: 9884/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 12 [0/60000 (0%)]#011Loss: 0.855865\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 12 [5120/60000 (8%)]#011Loss: 1.491695\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 12 [10240/60000 (17%)]#011Loss: 1.118722\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 12 [15360/60000 (25%)]#011Loss: 1.230382\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 12 [20480/60000 (34%)]#011Loss: 0.860174\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 12 [25600/60000 (42%)]#011Loss: 1.046342\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 12 [30720/60000 (51%)]#011Loss: 0.976646\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 12 [35840/60000 (59%)]#011Loss: 1.193576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 12 [40960/60000 (68%)]#011Loss: 0.871758\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 12 [46080/60000 (76%)]#011Loss: 0.869036\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 12 [51200/60000 (85%)]#011Loss: 1.190225\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 12 [56320/60000 (93%)]#011Loss: 1.190672\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Average loss: 0.0574, Accuracy: 9882/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 13 [0/60000 (0%)]#011Loss: 1.335586\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 13 [5120/60000 (8%)]#011Loss: 1.296896\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 13 [10240/60000 (17%)]#011Loss: 1.122913\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 13 [15360/60000 (25%)]#011Loss: 1.465160\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 13 [20480/60000 (34%)]#011Loss: 1.166382\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 13 [25600/60000 (42%)]#011Loss: 1.311949\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 13 [30720/60000 (51%)]#011Loss: 1.323143\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 13 [35840/60000 (59%)]#011Loss: 1.446222\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 13 [40960/60000 (68%)]#011Loss: 1.043971\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 13 [46080/60000 (76%)]#011Loss: 1.373976\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 13 [51200/60000 (85%)]#011Loss: 1.398304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 13 [56320/60000 (93%)]#011Loss: 1.164983\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Average loss: 0.0569, Accuracy: 9884/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 14 [0/60000 (0%)]#011Loss: 1.422917\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 14 [5120/60000 (8%)]#011Loss: 1.073740\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 14 [10240/60000 (17%)]#011Loss: 0.939431\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 14 [15360/60000 (25%)]#011Loss: 1.120518\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 14 [20480/60000 (34%)]#011Loss: 1.186857\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 14 [25600/60000 (42%)]#011Loss: 1.062768\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 14 [30720/60000 (51%)]#011Loss: 1.150235\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 14 [35840/60000 (59%)]#011Loss: 0.979159\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 14 [40960/60000 (68%)]#011Loss: 1.155477\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 14 [46080/60000 (76%)]#011Loss: 1.208133\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 14 [51200/60000 (85%)]#011Loss: 1.085077\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 14 [56320/60000 (93%)]#011Loss: 1.186224\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Average loss: 0.0570, Accuracy: 9884/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m2022-07-01 18:18:34,569 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-07-01 18:19:11 Uploading - Uploading generated training model\n",
      "2022-07-01 18:19:11 Completed - Training job completed\n",
      "ProfilerReport-1656698704: NoIssuesFound\n",
      "Training seconds: 352\n",
      "Billable seconds: 352\n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26124a71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
