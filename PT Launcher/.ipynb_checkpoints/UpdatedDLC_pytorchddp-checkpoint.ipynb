{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddb94cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (2.86.2)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker) (1.1.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: attrs==20.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker) (20.3.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: boto3>=1.20.21 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker) (1.21.42)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker) (3.19.1)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker) (1.19.2)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker) (3.7.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3>=1.20.21->sagemaker) (0.5.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3>=1.20.21->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.42 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3>=1.20.21->sagemaker) (1.24.42)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker) (2.4.7)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pandas->sagemaker) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pandas->sagemaker) (2021.1)\n",
      "Requirement already satisfied: dill>=0.3.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: pox>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pathos->sagemaker) (1.6.6.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from botocore<1.25.0,>=1.24.42->boto3>=1.20.21->sagemaker) (1.26.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8df1f2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::570106654206:role/Dev\n",
      "sagemaker bucket: sagemaker-us-west-2-570106654206\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "#Add instructions for local environment later, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac262828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\n",
      "\u001b[37m# Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file\u001b[39;49;00m\n",
      "\u001b[37m# except in compliance with the License. A copy of the License is located at\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\n",
      "\u001b[37m# http://aws.amazon.com/apache2.0/\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\n",
      "\u001b[37m# or in the \"LICENSE.txt\" file accompanying this file. This file is distributed on an \"AS IS\"\u001b[39;49;00m\n",
      "\u001b[37m# BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, express or implied. See the License for\u001b[39;49;00m\n",
      "\u001b[37m# the specific language governing permissions and limitations under the License.\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datasets, transforms\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlr_scheduler\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m StepLR\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mparallel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistributedDataParallel \u001b[34mas\u001b[39;49;00m DDP\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mdist\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msmdistributed\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdataparallel\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtorch_smddp\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Backend\n",
      "\u001b[33m'''\u001b[39;49;00m\n",
      "\u001b[33mFor EC2 conda env, in controller node run 'source /shared/pytorch_env/bin/activate', then launch the training using:\u001b[39;49;00m\n",
      "\u001b[33m/opt/amazon/openmpi/bin/mpirun --hostfile /shared/hostfile -N 1 --tag-output --bind-to none --oversubscribe \\\u001b[39;49;00m\n",
      "\u001b[33m--allow-run-as-root --mca btl_tcp_if_exclude lo,docker0 -x PATH -x LD_LIBRARY_PATH -x RDMAV_FORK_SAFE=1 \\\u001b[39;49;00m\n",
      "\u001b[33m-x NCCL_DEBUG=INFO -x FI_EFA_USE_DEVICE_RDMA=1 sh -c ' /shared/pytorch_env/bin/python \\\u001b[39;49;00m\n",
      "\u001b[33m-m torch.distributed.launch --nproc_per_node=8 --nnodes=`wc -l < /shared/hostname` --node_rank=$OMPI_COMM_WORLD_RANK \\\u001b[39;49;00m\n",
      "\u001b[33m--master_addr=`head -n 1 /shared/hostfile` --master_port=12358 /shared/ddp_mnist.py --data_loader_workers=8'\u001b[39;49;00m\n",
      "\u001b[33m'''\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mNet\u001b[39;49;00m(nn.Module):\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \u001b[36msuper\u001b[39;49;00m(Net, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\n",
      "        \u001b[36mself\u001b[39;49;00m.conv1 = nn.Conv2d(\u001b[34m1\u001b[39;49;00m, \u001b[34m32\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.conv2 = nn.Conv2d(\u001b[34m32\u001b[39;49;00m, \u001b[34m64\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.dropout1 = nn.Dropout2d(\u001b[34m0.25\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.dropout2 = nn.Dropout2d(\u001b[34m0.5\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.fc1 = nn.Linear(\u001b[34m9216\u001b[39;49;00m, \u001b[34m128\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.fc2 = nn.Linear(\u001b[34m128\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\n",
      "        x = \u001b[36mself\u001b[39;49;00m.conv1(x)\n",
      "        x = F.relu(x)\n",
      "        x = \u001b[36mself\u001b[39;49;00m.conv2(x)\n",
      "        x = F.relu(x)\n",
      "        x = F.max_pool2d(x, \u001b[34m2\u001b[39;49;00m)\n",
      "        x = \u001b[36mself\u001b[39;49;00m.dropout1(x)\n",
      "        x = torch.flatten(x, \u001b[34m1\u001b[39;49;00m)\n",
      "        x = \u001b[36mself\u001b[39;49;00m.fc1(x)\n",
      "        x = F.relu(x)\n",
      "        x = \u001b[36mself\u001b[39;49;00m.dropout2(x)\n",
      "        x = \u001b[36mself\u001b[39;49;00m.fc2(x)\n",
      "        output = F.log_softmax(x, dim=\u001b[34m1\u001b[39;49;00m)\n",
      "        \u001b[34mreturn\u001b[39;49;00m output\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args, model, device, train_loader, optimizer, epoch, rank):\n",
      "    model.train()\n",
      "    train_loader.sampler.set_epoch(epoch)\n",
      "    \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader):\n",
      "        data, target = data.to(device), target.to(device)\n",
      "        optimizer.zero_grad()\n",
      "        output = model(data)\n",
      "        loss = F.nll_loss(output, target)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        \u001b[34mif\u001b[39;49;00m batch_idx % args.log_interval == \u001b[34m0\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m rank == \u001b[34m0\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTrain Epoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m [\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)]\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33mLoss: \u001b[39;49;00m\u001b[33m{:.6f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "                epoch,\n",
      "                batch_idx * \u001b[36mlen\u001b[39;49;00m(data) * args.world_size,\n",
      "                \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\n",
      "                \u001b[34m100.\u001b[39;49;00m * batch_idx / \u001b[36mlen\u001b[39;49;00m(train_loader), loss.item()))\n",
      "        \u001b[34mif\u001b[39;49;00m args.verbose:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mBatch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, batch_idx, \u001b[33m\"\u001b[39;49;00m\u001b[33mfrom rank\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, rank)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(model, device, test_loader):\n",
      "    model.eval()\n",
      "    test_loss = \u001b[34m0\u001b[39;49;00m\n",
      "    correct = \u001b[34m0\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        \u001b[34mfor\u001b[39;49;00m data, target \u001b[35min\u001b[39;49;00m test_loader:\n",
      "            data, target = data.to(device), target.to(device)\n",
      "            output = model(data)\n",
      "            test_loss += F.nll_loss(\n",
      "                output, target, reduction=\u001b[33m'\u001b[39;49;00m\u001b[33msum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).item()  \u001b[37m# sum up batch loss\u001b[39;49;00m\n",
      "            pred = output.argmax(\n",
      "                dim=\u001b[34m1\u001b[39;49;00m,\n",
      "                keepdim=\u001b[34mTrue\u001b[39;49;00m)  \u001b[37m# get the index of the max log-probability\u001b[39;49;00m\n",
      "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
      "    test_loss /= \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mTest set: Average loss: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m, Accuracy: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "            test_loss, correct, \u001b[36mlen\u001b[39;49;00m(test_loader.dataset),\n",
      "            \u001b[34m100.\u001b[39;49;00m * correct / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)))\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\n",
      "    \u001b[37m# Training settings\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m'\u001b[39;49;00m\u001b[33mPyTorch MNIST Example\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m64\u001b[39;49;00m,\n",
      "                        metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m1000\u001b[39;49;00m,\n",
      "                        metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for testing (default: 1000)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m14\u001b[39;49;00m,\n",
      "                        metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 14)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "                        default=\u001b[34m1.0\u001b[39;49;00m,\n",
      "                        metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 1.0)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--gamma\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "                        default=\u001b[34m0.7\u001b[39;49;00m,\n",
      "                        metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mLearning rate step gamma (default: 0.7)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m1\u001b[39;49;00m,\n",
      "                        metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--local_rank\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=os.getenv(\u001b[33m'\u001b[39;49;00m\u001b[33mLOCAL_RANK\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, -\u001b[34m1\u001b[39;49;00m),\n",
      "                        metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m10\u001b[39;49;00m,\n",
      "        metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--save-model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        action=\u001b[33m'\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mFor Saving the current Model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33m--verbose\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "        action=\u001b[33m'\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "        default=\u001b[34mFalse\u001b[39;49;00m,\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mFor displaying smdistributed.dataparallel-specific logs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33m--data_loader_workers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m0\u001b[39;49;00m,\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mFor displaying smdistributed.dataparallel-specific logs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-path\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "                        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mPath for downloading \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mthe MNIST dataset\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    args = parser.parse_args()\n",
      "    args.world_size = \u001b[36mint\u001b[39;49;00m(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mWORLD_SIZE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    args.lr = \u001b[34m1.0\u001b[39;49;00m\n",
      "    args.batch_size //= args.world_size // \u001b[34m8\u001b[39;49;00m\n",
      "    args.batch_size = \u001b[36mmax\u001b[39;49;00m(args.batch_size, \u001b[34m1\u001b[39;49;00m)\n",
      "    data_path = args.data_path\n",
      "    dist.init_process_group(backend=\u001b[33m'\u001b[39;49;00m\u001b[33msmddp\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    rank = dist.get_rank()\n",
      "    local_rank = args.local_rank\n",
      "    torch.cuda.set_device(local_rank)\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m args.verbose:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mHello from rank\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, rank, \u001b[33m'\u001b[39;49;00m\u001b[33mof local_rank\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, local_rank,\n",
      "              \u001b[33m'\u001b[39;49;00m\u001b[33min world size of\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, args.world_size)\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m torch.cuda.is_available():\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mMust run smdistributed.dataparallel MNIST example on CUDA-capable devices.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        )\n",
      "    torch.manual_seed(args.seed)\n",
      "    device = torch.device(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mcuda:\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mlocal_rank\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[37m# select a single rank per node to download data\u001b[39;49;00m\n",
      "    is_first_local_rank = (local_rank == \u001b[34m0\u001b[39;49;00m)\n",
      "    \u001b[34mif\u001b[39;49;00m is_first_local_rank:\n",
      "        train_dataset = datasets.MNIST(data_path,\n",
      "                                       train=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                                       download=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                                       transform=transforms.Compose([\n",
      "                                           transforms.ToTensor(),\n",
      "                                           transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m, ),\n",
      "                                                                (\u001b[34m0.3081\u001b[39;49;00m, ))\n",
      "                                       ]))\n",
      "    dist.barrier()  \u001b[37m# prevent other ranks from accessing the data early\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m is_first_local_rank:\n",
      "        train_dataset = datasets.MNIST(data_path,\n",
      "                                       train=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                                       download=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                                       transform=transforms.Compose([\n",
      "                                           transforms.ToTensor(),\n",
      "                                           transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m, ),\n",
      "                                                                (\u001b[34m0.3081\u001b[39;49;00m, ))\n",
      "                                       ]))\n",
      "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
      "        train_dataset, num_replicas=args.world_size, rank=rank, shuffle=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        train_dataset,\n",
      "        batch_size=args.batch_size,\n",
      "        shuffle=\u001b[34mFalse\u001b[39;49;00m,\n",
      "        num_workers=args.data_loader_workers,\n",
      "        pin_memory=\u001b[34mTrue\u001b[39;49;00m,\n",
      "        sampler=train_sampler)\n",
      "    \u001b[34mif\u001b[39;49;00m rank == \u001b[34m0\u001b[39;49;00m:\n",
      "        test_loader = torch.utils.data.DataLoader(\n",
      "            datasets.MNIST(data_path,\n",
      "                           train=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                           transform=transforms.Compose([\n",
      "                               transforms.ToTensor(),\n",
      "                               transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m, ), (\u001b[34m0.3081\u001b[39;49;00m, ))\n",
      "                           ])),\n",
      "            batch_size=args.test_batch_size,\n",
      "            shuffle=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    model = Net()\n",
      "    model = model.to(device)\n",
      "    model = DDP(model, device_ids=[local_rank])\n",
      "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
      "    scheduler = StepLR(optimizer, step_size=\u001b[34m1\u001b[39;49;00m, gamma=args.gamma)\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, args.epochs + \u001b[34m1\u001b[39;49;00m):\n",
      "        train(args, model, device, train_loader, optimizer, epoch, rank)\n",
      "        \u001b[34mif\u001b[39;49;00m rank == \u001b[34m0\u001b[39;49;00m:\n",
      "            test(model, device, test_loader)\n",
      "        scheduler.step()\n",
      "    \u001b[34mif\u001b[39;49;00m args.save_model:\n",
      "        torch.save(model.state_dict(), \u001b[33m\"\u001b[39;49;00m\u001b[33mmnist_cnn.pt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/train_ptddp_mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "62ce9778",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_account_id = 763104351884  # By default, set the account ID used for most regions\n",
    "region = \"us-west-2\"\n",
    "image = (\n",
    "    \"ptddp_image\"  # Example: pt-smdataparallel-efficientnet-sagemaker\n",
    ")\n",
    "tag = \"latest\"  # Example: latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "052a5ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mARG\u001b[39;49;00m region\n",
      "\n",
      "\u001b[37m# Download base PT DLC. Note that this notebook requires a SM DLC with >= PT 1.10.2\u001b[39;49;00m\n",
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33m763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.11.0-gpu-py38-cu113-ubuntu20.04-sagemaker\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mARG\u001b[39;49;00m \u001b[31mWORK_DIR\u001b[39;49;00m=\u001b[33m\"ptddp_build\"\u001b[39;49;00m\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m $WORK_DIR\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m pwd; pip install git+https://github.com/vishwakaria/sagemaker-pytorch-training-toolkit/; \u001b[36mecho\u001b[39;49;00m \u001b[33m\"installed pt toolkit\"\u001b[39;49;00m; \u001b[33m\\\u001b[39;49;00m\n",
      "    \u001b[36mcd\u001b[39;49;00m sagemaker-pytorch-training-toolkit; \u001b[33m\\\u001b[39;49;00m\n",
      "    python setup.py; \u001b[33m\\\u001b[39;49;00m\n",
      "    \u001b[36mcd\u001b[39;49;00m ..; pip install git+https://github.com/vishwakaria/sagemaker-python-sdk/; \u001b[33m\\\u001b[39;49;00m\n",
      "    \u001b[36mcd\u001b[39;49;00m sagemaker-python-sdk; \u001b[33m\\\u001b[39;49;00m\n",
      "    python setup.py; \u001b[33m\\\u001b[39;49;00m\n",
      "    \u001b[36mcd\u001b[39;49;00m ../..; rm -rf \u001b[31m$WORK_DIR\u001b[39;49;00m;\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8964a71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m#!/usr/bin/env bash\u001b[39;49;00m\n",
      "\u001b[37m# This script shows how to build the Docker image and push it to ECR to be ready for use\u001b[39;49;00m\n",
      "\u001b[37m# by SageMaker.\u001b[39;49;00m\n",
      "\u001b[37m# The argument to this script is the image name. This will be used as the image on the local\u001b[39;49;00m\n",
      "\u001b[37m# machine and combined with the account and region to form the repository name for ECR.\u001b[39;49;00m\n",
      "\u001b[37m# set region\u001b[39;49;00m\n",
      "\n",
      "\u001b[31mDIR\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[34m$(\u001b[39;49;00m \u001b[36mcd\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[34m$(\u001b[39;49;00m dirname \u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mBASH_SOURCE\u001b[39;49;00m[0]\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34m)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m && \u001b[36mpwd\u001b[39;49;00m \u001b[34m)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[36mecho\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mDir: \u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mDIR\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[37m#cd ${DIR}/deberta\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[33m\"\u001b[39;49;00m\u001b[31m$#\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m -eq \u001b[34m3\u001b[39;49;00m ]; \u001b[34mthen\u001b[39;49;00m\n",
      "    \u001b[31mregion\u001b[39;49;00m=\u001b[31m$1\u001b[39;49;00m\n",
      "    \u001b[31mimage\u001b[39;49;00m=\u001b[31m$2\u001b[39;49;00m\n",
      "    \u001b[31mtag\u001b[39;49;00m=\u001b[31m$3\u001b[39;49;00m\n",
      "\u001b[34melse\u001b[39;49;00m\n",
      "    \u001b[36mecho\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33musage: \u001b[39;49;00m\u001b[31m$0\u001b[39;49;00m\u001b[33m <aws-region> \u001b[39;49;00m\u001b[31m$1\u001b[39;49;00m\u001b[33m <image-repo> \u001b[39;49;00m\u001b[31m$2\u001b[39;49;00m\u001b[33m <image-tag>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    \u001b[36mexit\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m\n",
      "\u001b[34mfi\u001b[39;49;00m\n",
      "\u001b[37m# Get the account number associated with the current IAM credentials\u001b[39;49;00m\n",
      "\u001b[31maccount\u001b[39;49;00m=\u001b[34m$(\u001b[39;49;00maws sts get-caller-identity --query Account --output text\u001b[34m)\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[31m$?\u001b[39;49;00m -ne \u001b[34m0\u001b[39;49;00m ]\n",
      "\u001b[34mthen\u001b[39;49;00m\n",
      "    \u001b[36mexit\u001b[39;49;00m \u001b[34m255\u001b[39;49;00m\n",
      "\u001b[34mfi\u001b[39;49;00m\n",
      "\n",
      "\u001b[31mfullname\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31maccount\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m.dkr.ecr.\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m.amazonaws.com/\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m:\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mtag\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[37m# If the repository doesn't exist in ECR, create it.\u001b[39;49;00m\n",
      "aws ecr describe-repositories --region \u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m --repository-names \u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m > /dev/null \u001b[34m2\u001b[39;49;00m>&\u001b[34m1\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[31m$?\u001b[39;49;00m -ne \u001b[34m0\u001b[39;49;00m ]; \u001b[34mthen\u001b[39;49;00m\n",
      "    \u001b[36mecho\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcreating ECR repository : \u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mfullname\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    aws ecr create-repository --region \u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m --repository-name \u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m > /dev/null\n",
      "\u001b[34mfi\u001b[39;49;00m\n",
      "\u001b[37m# Build the docker image locally with the image name and then push it to ECR\u001b[39;49;00m\n",
      "\u001b[37m# with the full name.\u001b[39;49;00m\n",
      "\u001b[37m# login ECR for the current account\u001b[39;49;00m\n",
      "\n",
      "aws ecr get-login-password --region \u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m | docker login --username AWS --password-stdin \u001b[33m${\u001b[39;49;00m\u001b[31maccount\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m.dkr.ecr.\u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m.amazonaws.com\n",
      "\n",
      "ls \u001b[33m${\u001b[39;49;00m\u001b[31mDIR\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\n",
      "docker build . -t \u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m -f \u001b[33m${\u001b[39;49;00m\u001b[31mDIR\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m/Dockerfile  --build-arg \u001b[31mregion\u001b[39;49;00m=\u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\n",
      "ls\n",
      "docker tag \u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m \u001b[33m${\u001b[39;49;00m\u001b[31mfullname\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\n",
      "docker push \u001b[33m${\u001b[39;49;00m\u001b[31mfullname\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[31m$?\u001b[39;49;00m -eq \u001b[34m0\u001b[39;49;00m ]; \u001b[34mthen\u001b[39;49;00m\n",
      "\t\u001b[36mecho\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mAmazon ECR URI: \u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mfullname\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[34melse\u001b[39;49;00m\n",
      "\t\u001b[36mecho\u001b[39;49;00m \u001b[33m\"Error: Image build and push failed\"\u001b[39;49;00m\n",
      "\t\u001b[36mexit\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m\n",
      "\u001b[34mfi\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./build_and_push.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "87b861b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted Images:\n",
      "untagged: 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.11.0-gpu-py38-cu113-ubuntu20.04-sagemaker\n",
      "untagged: 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training@sha256:b9526ba326845ca1e2aa103e53a49187d23fae902971786457d812c85829bf2c\n",
      "untagged: 570106654206.dkr.ecr.us-east-1.amazonaws.com/pt-ddp-launcher:latest\n",
      "untagged: 570106654206.dkr.ecr.us-east-1.amazonaws.com/pt-ddp-launcher@sha256:dc4c9dfd55054d64f86677985819f13a08da77a82131e9844a62d4d833f86f94\n",
      "untagged: pt-ddp-launcher:latest\n",
      "deleted: sha256:32f15c3fa7c191a7e91fa883e86c4ba5e15b40a2170e9bbd61b431a43fd08aff\n",
      "deleted: sha256:3e1392958553ea430bd343a86658be2b1b41ad90ccd277c7268672284035f71a\n",
      "untagged: 570106654206.dkr.ecr.us-east-1.amazonaws.com/pt-ddp-launcher-test:latest\n",
      "untagged: 570106654206.dkr.ecr.us-east-1.amazonaws.com/pt-ddp-launcher-test@sha256:e2fc4545e7eb062f99998dfab486a7ea6ff0e000c8bd59750cdacc378a9f33de\n",
      "untagged: pt-ddp-launcher-test:latest\n",
      "deleted: sha256:94b8ec1ab33a9342d2f5baa7a6914ea0031dd5d0cea71ace7b330b8888175573\n",
      "deleted: sha256:3c295bd3061517fc4ac1043ac76ae1188c37545dc544f6cbd7d55c139a11083d\n",
      "deleted: sha256:e16745430bb4566cdca35c9d107c34978f03d2e036c4e0ece4a292b578bea057\n",
      "deleted: sha256:53ccd9b18b522ac6ef65dbe06ce46fc28fee01b157c5b9490319b8a52b5f3bef\n",
      "untagged: 570106654206.dkr.ecr.us-east-1.amazonaws.com/pt-ddp-launcher@sha256:82e480a9c8e69bffb715a30d1282e020ef9f4e2aa2c79af6ae51fac0733ec1d5\n",
      "deleted: sha256:18233b239c554d3fe5f6723fea26af61faf3c0c1a24e28c399fdeeb773a5bb91\n",
      "deleted: sha256:0910de899eb12281e837e9ad59464ea53d1c52bb923c7cc8303e3db6b5984034\n",
      "untagged: 570106654206.dkr.ecr.us-east-1.amazonaws.com/pt-ddp-launcher@sha256:a3e10b20756f31bdca7208879dc75c57c5f52f7ddbb2aa1d8a31dcf9ba8644cb\n",
      "deleted: sha256:7160010e361c986957bf123df2c2eb661b35eab5628907287980361165b14f6c\n",
      "deleted: sha256:91de69eb9abfe8ec724d97dc74b26e999d6b90d43b6483f874b2ae97c43007de\n",
      "untagged: 570106654206.dkr.ecr.us-east-1.amazonaws.com/pt-ddp-launcher@sha256:d2e10a4797e0defb464944e9930a8b8b6c38da75cc871421bb42b487341a0ffa\n",
      "deleted: sha256:08e1ed4216c2811ee5d0e99a4ccb8317189ee1062cb2619e78f1e450cb2fbcf6\n",
      "deleted: sha256:a1c04dc87714e7b8a05cf55734ff0e64a6d24a5f167ea235a2f3645ee232ed61\n",
      "untagged: 570106654206.dkr.ecr.us-east-1.amazonaws.com/pt-ddp-launcher@sha256:199efa52531f7eb5e53c2dfff5d6210c88751698220b8c9f75dafdf502a52886\n",
      "deleted: sha256:6ede8e6f68aa9974e30d741c6a703156659d42c2e8f57b3a9de58da7c68f1753\n",
      "deleted: sha256:79b808695393b8b072817b41728b3927062176ecdf0909288f971be68bf4f840\n",
      "deleted: sha256:afd3b58652012f9e5e9d746c91b1b3c82e62d2c74b296b4f2a146b77504d3b72\n",
      "deleted: sha256:77c30d1cd68d9c4834c57d3174363656cb89ee74f93b2d633cce91c54454bde0\n",
      "deleted: sha256:f2577ff5fd3a51a518b330640d917ccc692bdfa01ef2c5a964372ec0a754c001\n",
      "deleted: sha256:374b9168d44b031b5678e4cdec3be4b6c7fa55308d544c69784e57dfd39bdc51\n",
      "deleted: sha256:c41badd7db3451839d9be9a1e38076b73fe44b9d65fbd63071c0064e0d7e6b94\n",
      "deleted: sha256:49059a5e690a6701e5b703966cac62ae1fb2c0ac7bf8f0295e3c05c846f7c36f\n",
      "deleted: sha256:7da00e148ecffd27e6ed8ab8b4088acd4e5d1c431fb3e9f0286fe1aad5b5e3ed\n",
      "deleted: sha256:c9caa1beb47a5e8dbbfc6d0af5555fdc10486dbb15f08158bb1f609fc36dd919\n",
      "deleted: sha256:fb178d4663aeafe2a59b7798f9bceacc275ae73f0f3783d27209a956179aca8f\n",
      "deleted: sha256:c5207e52ce960483c8a3543ec4781d40438ddfdf39625043d29684e160b2bf08\n",
      "deleted: sha256:44bc6946965c5808ee800ffe8dda24a8ed81609eaacb93948a6d0e769c95de1f\n",
      "deleted: sha256:f6c21614eb239eb44bc5ff447842b59f101658bef8c9d04d696cec1e649d2d68\n",
      "deleted: sha256:30754e78ee39dad9f3c73fcecd54b99cc9b6452d67c724c92b164919aa632149\n",
      "deleted: sha256:f878eab77e04fbb8022972a23613b2386077c4ba01c44bfb3c99619c8c52ca48\n",
      "deleted: sha256:413fa0461f5701142d3a02479d47da96d885bfe9333a828969697ccb7b36b71a\n",
      "deleted: sha256:03503799a1526e3db7f4bc5d09fe5358c537d7a7dd02f241caddbf7048bbb51e\n",
      "deleted: sha256:5c48e7a4c1bedb239e4350682f82b4d8b78bd25cd856cd72bd56fea43d1fed9a\n",
      "deleted: sha256:213a88348905ca66b8b35329666192ae4ad469f6bac32bddbac1c43dfad6f3c0\n",
      "deleted: sha256:c5e27c5b4ab1533a9acd7b01cb9b4f6448ac91a5f2c0e7076079d179bedf3165\n",
      "deleted: sha256:b32cd4c69682ae71ccc909727de9e53c9d5f79af98ed126d232167b425fa398f\n",
      "deleted: sha256:bf919c885292c8d1aac5090637667dd66d3a6922b379ebfe8436f06960c15ff2\n",
      "deleted: sha256:ad9e40e4b0657072c74b86b376d596ee9a1df7f9789f54a98241892f6d8714bc\n",
      "deleted: sha256:2caa2a2b5fa4016fa3c61e4cf907ee51415152e2af3d915af5f947a9de5cd7cd\n",
      "deleted: sha256:57827d98b1c9f3705a24e347d20d660a38b8e87f9a2c932186c56b8d1de8b1b4\n",
      "deleted: sha256:44c5742d17379e6e1591b65c1301fe35301e445b5019b9dda88b16e5031d7fcf\n",
      "deleted: sha256:e97501336af5e5fca6baba1d25e21808f2703e75664650b99cdc286febb21c14\n",
      "deleted: sha256:7de4abd54760144f5e339b93085b112acc666294e8e91d56f7b88ab41d15e138\n",
      "deleted: sha256:238dbacab38cd9aacd21316dcfc636b5385abdbd6a950bd09f60e1c7cb980292\n",
      "deleted: sha256:d6c04c21b2c1e448ebf7f536adb9763434c5c7276944ff1105f5e1e00238b32c\n",
      "deleted: sha256:8300d5fed5e5e9b2de625bfdc315f72fa150ba5f8d363449ac5e8da70960f68c\n",
      "deleted: sha256:9dff985e7e41168fca30e99624374d481c534f0bd8a10e84f81dc99db9b2b240\n",
      "deleted: sha256:e35ee636f2249f13694b723c445f42c0ff9e51ddbcf58ff6438ee9bbc8c7ecdd\n",
      "deleted: sha256:d31ce72a41e6a2129123578be259cc8ea74d6df58a2732a875fbd4edc587cef8\n",
      "deleted: sha256:d078f9e9809624fb32f29cbc6e67535b17108725b4b2639d2db196f36b4aaab8\n",
      "deleted: sha256:c3c9a2c2251eff97dc8b01ed41fdc68a7e8b30c8a6d334e5249faac036ee9f1d\n",
      "deleted: sha256:1259394243a422ff8e103fb6a0332ab13a3b3426b796ddb8b455108940e47012\n",
      "deleted: sha256:7994d8eb95e15ac9ff2b4b799ce533e1b3673f2ed9790bf6de928678c370ef83\n",
      "deleted: sha256:443e17d13a64234310241aa8de9cd6c0f371151b0cc26128e71bda502fb1db5f\n",
      "deleted: sha256:85839e81735e288d879b26e9d71c6a0e4082b6e39d2229a80720e9e04475af00\n",
      "deleted: sha256:c7cc43a80dfd894df76367f2aee324f67c35349538e98b5e94c5ec3c34783baf\n",
      "deleted: sha256:86b0fb4c74f1ec6d8e84be0be36d00bd7f6fb2e45cd4b908291cc96f6d9fcba2\n",
      "deleted: sha256:5a98400827b3eb8a13669361b173f317e68c1ca128d1373c5379fdab2960165a\n",
      "deleted: sha256:21e34a57a13dd633ee652d31b4a9d33acd55d01769a04eb23ca25d71272ca4a3\n",
      "deleted: sha256:cb5655baaf24e631cf6fd43adf1adb8e00c88cea82778b18e02a264723c35018\n",
      "deleted: sha256:9537597ae89a579a5e0ac675d99bb6f8e76ab0f15e4de167ad9250910b936e6a\n",
      "deleted: sha256:a90d5c984069e9f9f366e3993b2f26e1a96184818f129c3a567f67bfef3cff0d\n",
      "deleted: sha256:dc37dd9cada5492a121addef42a95038ff43e7ee64a34607e1fd905e01a4e392\n",
      "deleted: sha256:bf8cedc62fb3ef98ad0dff2be56ca451dd3ea69abd0031ad3e0fe5d9f9e4dfff\n",
      "\n",
      "Total reclaimed space: 14.32GB\n"
     ]
    }
   ],
   "source": [
    "! docker system prune -af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8de4367f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "region = \"us-west-2\"\n",
    "# image = (\n",
    "#     \"ptddp_image\"  # Example: pt-smdataparallel-efficientnet-sagemaker\n",
    "# )\n",
    "! aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 570106654206.dkr.ecr.{region}.amazonaws.com\n",
    "#! chmod +x build_and_push.sh\n",
    "#! bash build_and_push.sh {region} {image} {tag}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2fa8bb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# refer https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-training-containers to get the right uri's based on region\n",
    "#image_uri = '763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.9.1-transformers4.12.3-gpu-py38-cu111-ubuntu20.04'\n",
    "image_uri = '570106654206.dkr.ecr.us-west-2.amazonaws.com/ptddp-launcher:latest'\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "# this is the only line of code change required to leverage SageMaker Distributed Data Parallel\n",
    "distribution = {'pytorchddp':{ 'enabled': True }}\n",
    "#distribution = {\"mpi\":{\"enabled\":True, \"num_of_processes_per_host\":8}}\n",
    "\n",
    "\n",
    "estimator = PyTorch(\n",
    "    base_job_name=\"ptddp-mnist-test\",\n",
    "    source_dir=\"code\",\n",
    "    entry_point=\"train_ptddp_mnist.py\",\n",
    "    role=role,\n",
    "    framework_version=\"1.11.0\",\n",
    "    py_version=\"py38\",\n",
    "    image_uri=image_uri,\n",
    "    # For training with multinode distributed training, set this count. Example: 2\n",
    "    instance_count=1,\n",
    "    # For training with p3dn instance use - ml.p3dn.24xlarge, with p4dn instance use - ml.p4d.24xlarge\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    sagemaker_session=sess,\n",
    "    # Training using SMDataParallel Distributed Training Framework\n",
    "    distribution=distribution,\n",
    "    debugger_hook_config=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ae1fcc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-28 05:41:08 Starting - Starting the training job...ProfilerReport-1656394868: InProgress\n",
      "...\n",
      "2022-06-28 05:41:51 Starting - Preparing the instances for training..........................................\n",
      "2022-06-28 05:49:10 Downloading - Downloading input data\n",
      "2022-06-28 05:49:10 Training - Downloading the training image...........................\n",
      "2022-06-28 05:53:36 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34msed: can't read changehostname.c: No such file or directory\u001b[0m\n",
      "\u001b[34mgcc: error: changehostname.c: No such file or directory\u001b[0m\n",
      "\u001b[34mgcc: fatal error: no input files\u001b[0m\n",
      "\u001b[34mcompilation terminated.\u001b[0m\n",
      "\u001b[34mgcc: error: changehostname.o: No such file or directory\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/paramiko/transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m2022-06-28 05:53:38,979 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m2022-06-28 05:53:39,060 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-06-28 05:53:39,065 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m2022-06-28 05:53:39,695 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"ptddp-mnist-test-2022-06-28-05-41-08-016\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-06-28-05-41-08-016/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_ptddp_mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_ptddp_mnist.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_ptddp_mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_ptddp_mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-06-28-05-41-08-016/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"ptddp-mnist-test-2022-06-28-05-41-08-016\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-06-28-05-41-08-016/source/sourcedir.tar.gz\",\"module_name\":\"train_ptddp_mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_ptddp_mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220430-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train_ptddp_mnist.py\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"train_ptddp_mnist.py\", line 244, in <module>\u001b[0m\n",
      "\u001b[34mmain()\n",
      "  File \"train_ptddp_mnist.py\", line 169, in main\n",
      "    args.world_size = int(os.environ['WORLD_SIZE'])\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/os.py\", line 675, in __getitem__\u001b[0m\n",
      "\u001b[34mraise KeyError(key) from None\u001b[0m\n",
      "\u001b[34mKeyError: 'WORLD_SIZE'\u001b[0m\n",
      "\u001b[34m2022-06-28 05:53:42,762 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2022-06-28 05:53:42,762 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"raise KeyError(key) from None\n",
      " KeyError: 'WORLD_SIZE'\"\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.8 train_ptddp_mnist.py\"\u001b[0m\n",
      "\u001b[34m2022-06-28 05:53:42,762 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2022-06-28 05:54:11 Uploading - Uploading generated training model\n",
      "2022-06-28 05:54:11 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job ptddp-mnist-test-2022-06-28-05-41-08-016: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"raise KeyError(key) from None\n KeyError: 'WORLD_SIZE'\"\nCommand \"/opt/conda/bin/python3.8 train_ptddp_mnist.py\", exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-28be9b2c12b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1956\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1957\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3797\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3798\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3799\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3800\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3337\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3338\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3339\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3340\u001b[0m             )\n\u001b[1;32m   3341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job ptddp-mnist-test-2022-06-28-05-41-08-016: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"raise KeyError(key) from None\n KeyError: 'WORLD_SIZE'\"\nCommand \"/opt/conda/bin/python3.8 train_ptddp_mnist.py\", exit code: 1"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7d597e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-27 18:51:23 Starting - Starting the training job...ProfilerReport-1656355883: InProgress\n",
      "...\n",
      "2022-06-27 18:52:12 Starting - Preparing the instances for training..........................................\n",
      "2022-06-27 18:59:20 Downloading - Downloading input data...\n",
      "2022-06-27 18:59:51 Training - Downloading the training image..........................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34msed: can't read changehostname.c: No such file or directory\u001b[0m\n",
      "\u001b[34mgcc: error: changehostname.c: No such file or directory\u001b[0m\n",
      "\u001b[34mgcc: fatal error: no input files\u001b[0m\n",
      "\u001b[34mcompilation terminated.\u001b[0m\n",
      "\u001b[34mgcc: error: changehostname.o: No such file or directory\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-training-toolkit:No exception classes found in smdistributed.dataparallel\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-training-toolkit:Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_pytorch_container.training:Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_pytorch_container.training:Invoking user training script.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-training-toolkit:Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"ptddp-mnist-test-2022-06-27-18-51-23-608\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-06-27-18-51-23-608/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_ptddp_mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_ptddp_mnist.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_ptddp_mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_ptddp_mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-06-27-18-51-23-608/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"ptddp-mnist-test-2022-06-27-18-51-23-608\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-570106654206/ptddp-mnist-test-2022-06-27-18-51-23-608/source/sourcedir.tar.gz\",\"module_name\":\"train_ptddp_mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_ptddp_mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.16b20220617-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train_ptddp_mnist.py\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"train_ptddp_mnist.py\", line 244, in <module>\u001b[0m\n",
      "\u001b[34mmain()\n",
      "  File \"train_ptddp_mnist.py\", line 169, in main\n",
      "    args.world_size = int(os.environ['WORLD_SIZE'])\n",
      "  File \"/opt/conda/lib/python3.8/os.py\", line 675, in __getitem__\u001b[0m\n",
      "\u001b[34mraise KeyError(key) from None\u001b[0m\n",
      "\u001b[34mKeyError: 'WORLD_SIZE'\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-training-toolkit:Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-training-toolkit:Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[34mERROR:sagemaker-training-toolkit:Reporting training FAILURE\u001b[0m\n",
      "\u001b[34mERROR:sagemaker-training-toolkit:ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"raise KeyError(key) from None\n",
      " KeyError: 'WORLD_SIZE'\"\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.8 train_ptddp_mnist.py\"\u001b[0m\n",
      "\u001b[34mERROR:sagemaker-training-toolkit:Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2022-06-27 19:04:23 Uploading - Uploading generated training model\n",
      "2022-06-27 19:04:23 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job ptddp-mnist-test-2022-06-27-18-51-23-608: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"raise KeyError(key) from None\n KeyError: 'WORLD_SIZE'\"\nCommand \"/opt/conda/bin/python3.8 train_ptddp_mnist.py\", exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-58808bd1742f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Running with backend = nccl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1956\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1957\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3797\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3798\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3799\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3800\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3337\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3338\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3339\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3340\u001b[0m             )\n\u001b[1;32m   3341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job ptddp-mnist-test-2022-06-27-18-51-23-608: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"raise KeyError(key) from None\n KeyError: 'WORLD_SIZE'\"\nCommand \"/opt/conda/bin/python3.8 train_ptddp_mnist.py\", exit code: 1"
     ]
    }
   ],
   "source": [
    "## Running with backend = nccl\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c7aa14c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "region = \"us-west-2\"\n",
    "! aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 570106654206.dkr.ecr.{region}.amazonaws.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9235c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test if normal docker container works with this script\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# refer https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-training-containers to get the right uri's based on region\n",
    "image_uri = '763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:1.11.0-gpu-py38-cu113-ubuntu20.04-sagemaker'\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "# this is the only line of code change required to leverage SageMaker Distributed Data Parallel\n",
    "distribution = { \"smdistributed\": { \"dataparallel\": { \"enabled\": True } } }\n",
    "#distribution = {\"mpi\":{\"enabled\":True, \"num_of_processes_per_host\":8}}\n",
    "\n",
    "\n",
    "estimator_pt_base = PyTorch(\n",
    "    base_job_name=\"ptddp-mnist-test\",\n",
    "    source_dir=\"code\",\n",
    "    entry_point=\"train_ptddp_mnist.py\",\n",
    "    role=role,\n",
    "    framework_version=\"1.11.0\",\n",
    "    py_version=\"py38\",\n",
    "    image_uri=image_uri,\n",
    "    # For training with multinode distributed training, set this count. Example: 2\n",
    "    instance_count=1,\n",
    "    # For training with p3dn instance use - ml.p3dn.24xlarge, with p4dn instance use - ml.p4d.24xlarge\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    sagemaker_session=sess,\n",
    "    # Training using SMDataParallel Distributed Training Framework\n",
    "    distribution=distribution,\n",
    "    debugger_hook_config=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2923957b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-28 23:59:26 Starting - Starting the training job...ProfilerReport-1656460766: InProgress\n",
      "...\n",
      "2022-06-29 00:00:22 Starting - Preparing the instances for training................."
     ]
    }
   ],
   "source": [
    "estimator_pt_base.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f0011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
